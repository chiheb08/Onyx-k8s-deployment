# Hugging Face Models Flow in Onyx Model Servers

Complete visual explanation of how model servers download, cache, and use Hugging Face models.

---

## ğŸ—ï¸ High-Level Architecture: Model Servers & Hugging Face

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ğŸŒ HUGGING FACE HUB                              â”‚
â”‚                     https://huggingface.co/                             â”‚
â”‚                                                                         â”‚
â”‚  ğŸ“¦ Models Repository:                                                  â”‚
â”‚  â”œâ”€ nomic-ai/nomic-embed-text-v1 (~1.5GB)                             â”‚
â”‚  â”œâ”€ mixedbread-ai/mxbai-rerank-xsmall-v1 (~200MB)                     â”‚
â”‚  â”œâ”€ onyx-dot-app/hybrid-intent-token-classifier (~100MB)              â”‚
â”‚  â”œâ”€ onyx-dot-app/information-content-model (~100MB)                   â”‚
â”‚  â”œâ”€ distilbert-base-uncased (tokenizer)                               â”‚
â”‚  â””â”€ intfloat/e5-base-v2, intfloat/e5-small-v2, etc.                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â”‚ (Download during Docker build
                                    â”‚  + runtime on first use)
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               ğŸ³ DOCKER IMAGE BUILD (Dockerfile.model_server)           â”‚
â”‚                                                                         â”‚
â”‚  Pre-download Models:                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ RUN python -c "                                            â”‚        â”‚
â”‚  â”‚   from huggingface_hub import snapshot_download;          â”‚        â”‚
â”‚  â”‚   snapshot_download('nomic-ai/nomic-embed-text-v1');     â”‚        â”‚
â”‚  â”‚   snapshot_download('mixedbread-ai/mxbai-rerank-xsmall-v1');â”‚     â”‚
â”‚  â”‚   snapshot_download('onyx-dot-app/hybrid-intent-...');    â”‚        â”‚
â”‚  â”‚   snapshot_download('onyx-dot-app/information-content-...');â”‚      â”‚
â”‚  â”‚   ...                                                      â”‚        â”‚
â”‚  â”‚ "                                                          â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                                         â”‚
â”‚  Result: Models stored in /app/.cache/temp_huggingface/                â”‚
â”‚  Image Size: ~3GB per model server image                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â”‚ (Deploy image)
                                    â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                                                  â”‚
         â–¼                                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ¤– INFERENCE MODEL    â”‚                    â”‚  ğŸ¤– INDEXING MODEL     â”‚
â”‚     SERVER             â”‚                    â”‚     SERVER             â”‚
â”‚                        â”‚                    â”‚                        â”‚
â”‚  Container 1           â”‚                    â”‚  Container 2           â”‚
â”‚  Port: 9000            â”‚                    â”‚  Port: 9000            â”‚
â”‚  Env: (none)           â”‚                    â”‚  Env: INDEXING_ONLY=Trueâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                                  â”‚
         â”‚                                                  â”‚
         â–¼                                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ’¾ MODEL CACHE        â”‚                    â”‚  ğŸ’¾ MODEL CACHE        â”‚
â”‚  VOLUME                â”‚                    â”‚  VOLUME                â”‚
â”‚                        â”‚                    â”‚                        â”‚
â”‚  /app/.cache/          â”‚                    â”‚  /app/.cache/          â”‚
â”‚  huggingface/          â”‚                    â”‚  huggingface/          â”‚
â”‚                        â”‚                    â”‚                        â”‚
â”‚  ğŸ“ models--nomic-ai-- â”‚                    â”‚  ğŸ“ models--nomic-ai-- â”‚
â”‚     nomic-embed-text-v1â”‚                    â”‚     nomic-embed-text-v1â”‚
â”‚  ğŸ“ models--mixedbread-â”‚                    â”‚  ğŸ“ models--mixedbread-â”‚
â”‚     ai--mxbai-rerank-  â”‚                    â”‚     ai--mxbai-rerank-  â”‚
â”‚     xsmall-v1          â”‚                    â”‚     xsmall-v1          â”‚
â”‚  ğŸ“ models--onyx-dot-  â”‚                    â”‚  ğŸ“ models--onyx-dot-  â”‚
â”‚     app--hybrid-intent-â”‚                    â”‚     app--information-  â”‚
â”‚     token-classifier   â”‚                    â”‚     content-model      â”‚
â”‚                        â”‚                    â”‚                        â”‚
â”‚  Size: ~2-3GB          â”‚                    â”‚  Size: ~2-3GB          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                                  â”‚
         â”‚ (Models loaded into memory)                     â”‚
         â”‚                                                  â”‚
         â–¼                                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ§  IN-MEMORY MODELS   â”‚                    â”‚  ğŸ§  IN-MEMORY MODELS   â”‚
â”‚                        â”‚                    â”‚                        â”‚
â”‚  â€¢ Embedding Model     â”‚                    â”‚  â€¢ Embedding Model     â”‚
â”‚    (nomic-embed-text)  â”‚                    â”‚    (nomic-embed-text)  â”‚
â”‚  â€¢ Reranking Model     â”‚                    â”‚  â€¢ Reranking Model     â”‚
â”‚    (mxbai-rerank)      â”‚                    â”‚    (mxbai-rerank)      â”‚
â”‚  â€¢ Intent Classifier   â”‚                    â”‚  â€¢ Content Classifier  â”‚
â”‚    (hybrid-intent)     â”‚                    â”‚    (info-content)      â”‚
â”‚                        â”‚                    â”‚                        â”‚
â”‚  RAM Usage: ~2GB       â”‚                    â”‚  RAM Usage: ~2GB       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                                  â”‚
         â”‚                                                  â”‚
         â–¼                                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ”„ PROCESSES REQUESTS â”‚                    â”‚  ğŸ”„ PROCESSES REQUESTS â”‚
â”‚                        â”‚                    â”‚                        â”‚
â”‚  Used by:              â”‚                    â”‚  Used by:              â”‚
â”‚  â€¢ API Server          â”‚                    â”‚  â€¢ Background Workers  â”‚
â”‚                        â”‚                    â”‚                        â”‚
â”‚  Purpose:              â”‚                    â”‚  Purpose:              â”‚
â”‚  â€¢ User query â†’        â”‚                    â”‚  â€¢ Document chunks â†’   â”‚
â”‚    embeddings          â”‚                    â”‚    embeddings          â”‚
â”‚  â€¢ Real-time search    â”‚                    â”‚  â€¢ Bulk indexing       â”‚
â”‚                        â”‚                    â”‚                        â”‚
â”‚  Workload:             â”‚                    â”‚  Workload:             â”‚
â”‚  â€¢ Single queries      â”‚                    â”‚  â€¢ Batch processing    â”‚
â”‚  â€¢ Low latency         â”‚                    â”‚  â€¢ High throughput     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¥ Download & Caching Flow

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                     FIRST TIME STARTUP (NO CACHE)                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Step 1: Container Starts
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Container: inference-model-server                                  â”‚
â”‚  Status: Starting...                                                â”‚
â”‚  Volume: Empty (or contains pre-downloaded models from image)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
Step 2: Check Cache Directory
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Path: /app/.cache/huggingface/                                     â”‚
â”‚  Status: Empty (if using emptyDir) OR Has pre-built models          â”‚
â”‚                                                                     â”‚
â”‚  IF pre-built models exist:                                         â”‚
â”‚    Move from /app/.cache/temp_huggingface/ â†’ /app/.cache/huggingface/â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
Step 3: Model Loading (SentenceTransformer)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Code: model = SentenceTransformer('nomic-ai/nomic-embed-text-v1') â”‚
â”‚                                                                     â”‚
â”‚  Check 1: Local cache exists?                                       â”‚
â”‚  â”œâ”€ YES â†’ Load from /app/.cache/huggingface/                       â”‚
â”‚  â””â”€ NO  â†’ Download from Hugging Face Hub                           â”‚
â”‚                                                                     â”‚
â”‚  Download Process (if needed):                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ 1. Connect to https://huggingface.co/                         â”‚ â”‚
â”‚  â”‚ 2. Download model files:                                      â”‚ â”‚
â”‚  â”‚    â€¢ config.json                                              â”‚ â”‚
â”‚  â”‚    â€¢ model.safetensors (or pytorch_model.bin)                 â”‚ â”‚
â”‚  â”‚    â€¢ tokenizer_config.json                                    â”‚ â”‚
â”‚  â”‚    â€¢ special_tokens_map.json                                  â”‚ â”‚
â”‚  â”‚    â€¢ vocab.txt                                                â”‚ â”‚
â”‚  â”‚ 3. Save to: /app/.cache/huggingface/models--nomic-ai--nomic-...â”‚ â”‚
â”‚  â”‚ 4. Total size: ~1.5GB                                         â”‚ â”‚
â”‚  â”‚ 5. Time: 2-10 minutes (depends on internet speed)             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
Step 4: Model Warmup
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pre-warm RoPE caches:                                              â”‚
â”‚  â€¢ Generate dummy text                                              â”‚
â”‚  â€¢ Process through model                                            â”‚
â”‚  â€¢ Cache rotary position embeddings                                 â”‚
â”‚  â€¢ Result: Faster inference on real queries                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
Step 5: Ready to Serve
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Status: READY âœ…                                                   â”‚
â”‚  Endpoints:                                                         â”‚
â”‚  â€¢ POST /embed - Generate embeddings                                â”‚
â”‚  â€¢ POST /rerank - Rerank search results                             â”‚
â”‚  â€¢ GET /health - Health check                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    SUBSEQUENT STARTUPS (WITH CACHE)                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Step 1: Container Starts
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Container: inference-model-server                                  â”‚
â”‚  Status: Starting...                                                â”‚
â”‚  Volume: Contains cached models âœ…                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
Step 2: Check Cache Directory
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Path: /app/.cache/huggingface/                                     â”‚
â”‚  Status: Models found! âœ…                                           â”‚
â”‚  â€¢ models--nomic-ai--nomic-embed-text-v1/                           â”‚
â”‚  â€¢ models--mixedbread-ai--mxbai-rerank-xsmall-v1/                   â”‚
â”‚  â€¢ models--onyx-dot-app--hybrid-intent-token-classifier/            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
Step 3: Model Loading (Fast!)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Code: model = SentenceTransformer('nomic-ai/nomic-embed-text-v1') â”‚
â”‚                                                                     â”‚
â”‚  Check: Local cache exists? âœ… YES                                  â”‚
â”‚  Action: Load from /app/.cache/huggingface/ (no download!)         â”‚
â”‚                                                                     â”‚
â”‚  Time: ~30-60 seconds (just loading into memory)                    â”‚
â”‚  No internet required! âœ…                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
Step 4: Model Warmup
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pre-warm RoPE caches: ~10-20 seconds                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
Step 5: Ready to Serve
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Status: READY âœ…                                                   â”‚
â”‚  Total Time: ~1-2 minutes (vs 5-15 minutes first time)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Request Processing Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     USER SEARCH QUERY EXAMPLE                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User types: "What is our vacation policy?"
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API Server receives query                                          â”‚
â”‚  Needs to convert text â†’ embedding vector                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API Server â†’ Inference Model Server                                â”‚
â”‚  POST http://inference-model-server:9000/embed                      â”‚
â”‚  Body: {                                                            â”‚
â”‚    "texts": ["What is our vacation policy?"],                       â”‚
â”‚    "model_name": "nomic-ai/nomic-embed-text-v1",                   â”‚
â”‚    "max_context_length": 512                                        â”‚
â”‚  }                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Inference Model Server processes:                                  â”‚
â”‚                                                                     â”‚
â”‚  1. Load model from memory (already loaded)                         â”‚
â”‚     â”œâ”€ Model: nomic-ai/nomic-embed-text-v1                         â”‚
â”‚     â””â”€ Status: Cached in RAM âœ…                                     â”‚
â”‚                                                                     â”‚
â”‚  2. Tokenize text                                                   â”‚
â”‚     â”œâ”€ Input: "What is our vacation policy?"                        â”‚
â”‚     â””â”€ Output: [101, 2054, 2003, 2256, 5840, 3343, 1029, 102]      â”‚
â”‚                                                                     â”‚
â”‚  3. Run through neural network (Hugging Face model)                 â”‚
â”‚     â”œâ”€ Input tokens â†’ Transformer layers                            â”‚
â”‚     â”œâ”€ Process with nomic-embed-text-v1 weights                     â”‚
â”‚     â””â”€ Output: 768-dimensional vector                               â”‚
â”‚                                                                     â”‚
â”‚  4. Normalize embeddings                                            â”‚
â”‚     â”œâ”€ Apply L2 normalization                                       â”‚
â”‚     â””â”€ Result: Unit vector for cosine similarity                    â”‚
â”‚                                                                     â”‚
â”‚  Time: ~100ms                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Return embedding vector:                                           â”‚
â”‚  [0.123, -0.456, 0.789, ..., 0.321] (768 values)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API Server uses embedding for:                                     â”‚
â”‚  â€¢ Vector search in Vespa                                           â”‚
â”‚  â€¢ Find similar document chunks                                     â”‚
â”‚  â€¢ Retrieve relevant context                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DOCUMENT INDEXING EXAMPLE                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User uploads: "HR_Policy_2025.pdf"
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Background Worker receives task:                                   â”‚
â”‚  â€¢ Extract text from PDF                                            â”‚
â”‚  â€¢ Result: 50 pages of text                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Chunk document:                                                    â”‚
â”‚  â€¢ Split into 200 chunks (~512 tokens each)                         â”‚
â”‚  â€¢ Each chunk needs embedding                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Background Worker â†’ Indexing Model Server                          â”‚
â”‚  POST http://indexing-model-server:9000/embed                       â”‚
â”‚  Body: {                                                            â”‚
â”‚    "texts": ["Chunk 1...", "Chunk 2...", ... "Chunk 200..."],      â”‚
â”‚    "model_name": "nomic-ai/nomic-embed-text-v1",                   â”‚
â”‚    "max_context_length": 512,                                       â”‚
â”‚    "batch_size": 8                                                  â”‚
â”‚  }                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Indexing Model Server processes (BATCH):                           â”‚
â”‚                                                                     â”‚
â”‚  1. Load model from memory (already loaded)                         â”‚
â”‚     â”œâ”€ Model: nomic-ai/nomic-embed-text-v1                         â”‚
â”‚     â”œâ”€ Same model as Inference Server!                              â”‚
â”‚     â””â”€ But dedicated for bulk processing                            â”‚
â”‚                                                                     â”‚
â”‚  2. Process in batches (8 chunks at a time)                         â”‚
â”‚     â”œâ”€ Batch 1: Chunks 1-8                                          â”‚
â”‚     â”œâ”€ Batch 2: Chunks 9-16                                         â”‚
â”‚     â”œâ”€ ...                                                          â”‚
â”‚     â””â”€ Batch 25: Chunks 193-200                                     â”‚
â”‚                                                                     â”‚
â”‚  3. For each batch:                                                 â”‚
â”‚     â”œâ”€ Tokenize all texts                                           â”‚
â”‚     â”œâ”€ Run through Hugging Face model                               â”‚
â”‚     â”œâ”€ Generate 768-dim vectors                                     â”‚
â”‚     â””â”€ Normalize embeddings                                         â”‚
â”‚                                                                     â”‚
â”‚  Time: ~30-60 seconds for 200 chunks                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Return 200 embedding vectors:                                      â”‚
â”‚  [                                                                  â”‚
â”‚    [0.123, -0.456, ...], // Chunk 1                                â”‚
â”‚    [0.789, 0.321, ...],  // Chunk 2                                â”‚
â”‚    ...                                                              â”‚
â”‚    [0.654, -0.987, ...]  // Chunk 200                              â”‚
â”‚  ]                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Background Worker stores in Vespa:                                 â”‚
â”‚  â€¢ Each chunk + embedding â†’ Vespa document                          â”‚
â”‚  â€¢ Document ready for search!                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—‚ï¸ File System Layout

```
Container: inference-model-server
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

/app/
â”œâ”€â”€ .cache/
â”‚   â”œâ”€â”€ temp_huggingface/  (moved to huggingface/ on startup)
â”‚   â”‚   â”œâ”€â”€ models--nomic-ai--nomic-embed-text-v1/
â”‚   â”‚   â”œâ”€â”€ models--mixedbread-ai--mxbai-rerank-xsmall-v1/
â”‚   â”‚   â””â”€â”€ models--onyx-dot-app--hybrid-intent-token-classifier/
â”‚   â”‚
â”‚   â””â”€â”€ huggingface/  (active cache, mounted volume)
â”‚       â”œâ”€â”€ models--nomic-ai--nomic-embed-text-v1/
â”‚       â”‚   â”œâ”€â”€ snapshots/
â”‚       â”‚   â”‚   â””â”€â”€ abc123def456.../
â”‚       â”‚   â”‚       â”œâ”€â”€ config.json
â”‚       â”‚   â”‚       â”œâ”€â”€ model.safetensors (1.2GB)
â”‚       â”‚   â”‚       â”œâ”€â”€ tokenizer_config.json
â”‚       â”‚   â”‚       â”œâ”€â”€ special_tokens_map.json
â”‚       â”‚   â”‚       â”œâ”€â”€ vocab.txt
â”‚       â”‚   â”‚       â””â”€â”€ tokenizer.json
â”‚       â”‚   â””â”€â”€ refs/
â”‚       â”‚       â””â”€â”€ main â†’ abc123def456...
â”‚       â”‚
â”‚       â”œâ”€â”€ models--mixedbread-ai--mxbai-rerank-xsmall-v1/
â”‚       â”‚   â””â”€â”€ snapshots/
â”‚       â”‚       â””â”€â”€ xyz789ghi012.../
â”‚       â”‚           â”œâ”€â”€ config.json
â”‚       â”‚           â”œâ”€â”€ model.safetensors (180MB)
â”‚       â”‚           â””â”€â”€ ...
â”‚       â”‚
â”‚       â””â”€â”€ models--onyx-dot-app--hybrid-intent-token-classifier/
â”‚           â””â”€â”€ snapshots/
â”‚               â””â”€â”€ def456abc789.../
â”‚                   â”œâ”€â”€ config.json
â”‚                   â”œâ”€â”€ pytorch_model.bin (95MB)
â”‚                   â””â”€â”€ ...
â”‚
â”œâ”€â”€ model_server/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ encoders.py
â”‚   â”œâ”€â”€ custom_models.py
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ shared_configs/
    â””â”€â”€ configs.py

Volume Mount:
â•â•â•â•â•â•â•â•â•â•â•â•â•
Docker Compose: model_cache_huggingface â†’ /app/.cache/huggingface/
Kubernetes: emptyDir â†’ /app/.cache/huggingface/ (or persistent PVC)
```

---

## ğŸ’¾ Storage Comparison: Docker vs Kubernetes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DOCKER COMPOSE                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

volumes:
  model_cache_huggingface:           # Named volume (persistent)
  indexing_huggingface_model_cache:  # Named volume (persistent)

Behavior:
â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Models downloaded once
â€¢ Cached across container restarts âœ…
â€¢ Shared between container recreations âœ…
â€¢ Persists even if container deleted âœ…
â€¢ Only deleted if volume explicitly removed

Location on host:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
/var/lib/docker/volumes/onyx_model_cache_huggingface/_data/


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      KUBERNETES (CURRENT)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

volumes:
  - name: inference-model-cache
    emptyDir: {}                     # Temporary volume

  - name: indexing-model-cache
    emptyDir: {}                     # Temporary volume

Behavior:
â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Models downloaded on every pod start âŒ
â€¢ Lost when pod deleted/restarted âŒ
â€¢ Not shared between pods âŒ
â€¢ Wastes bandwidth and time âŒ

âš ï¸  PROBLEM: Re-downloads ~2-3GB on every restart!


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                KUBERNETES (RECOMMENDED FOR PRODUCTION)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

volumes:
  - name: inference-model-cache
    persistentVolumeClaim:
      claimName: inference-model-cache-pvc

  - name: indexing-model-cache
    persistentVolumeClaim:
      claimName: indexing-model-cache-pvc

PVC Configuration:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: inference-model-cache-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: "nfs-example"
  volumeMode: "Filesystem"
  resources:
    requests:
      storage: 5Gi

Behavior:
â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Models downloaded once âœ…
â€¢ Cached across pod restarts âœ…
â€¢ Persists when pod deleted âœ…
â€¢ Saves bandwidth and time âœ…
â€¢ Can be shared (with ReadWriteMany) âœ…
```

---

## ğŸ“Š Model Size & Resource Summary

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                        MODEL SIZES & RESOURCES                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model: nomic-ai/nomic-embed-text-v1 (DEFAULT EMBEDDING MODEL)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Disk Size:        ~1.5 GB                                            â”‚
â”‚  RAM (loaded):     ~2.0 GB                                            â”‚
â”‚  Dimensions:       768                                                â”‚
â”‚  Max Context:      512 tokens                                         â”‚
â”‚  License:          Apache 2.0                                         â”‚
â”‚  Purpose:          Convert text â†’ embeddings                          â”‚
â”‚  Used by:          Both servers (inference + indexing)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model: mixedbread-ai/mxbai-rerank-xsmall-v1 (RERANKING MODEL)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Disk Size:        ~200 MB                                            â”‚
â”‚  RAM (loaded):     ~300 MB                                            â”‚
â”‚  Purpose:          Rerank search results by relevance                 â”‚
â”‚  Used by:          Both servers                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model: onyx-dot-app/hybrid-intent-token-classifier                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Disk Size:        ~100 MB                                            â”‚
â”‚  RAM (loaded):     ~150 MB                                            â”‚
â”‚  Purpose:          Classify user intent (search vs chat)              â”‚
â”‚  Used by:          Inference server only                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model: onyx-dot-app/information-content-model                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Disk Size:        ~100 MB                                            â”‚
â”‚  RAM (loaded):     ~150 MB                                            â”‚
â”‚  Purpose:          Score information density of text chunks           â”‚
â”‚  Used by:          Indexing server only                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

TOTAL RESOURCES PER SERVER:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Inference Model Server:
â€¢ Disk: ~2.0 GB
â€¢ RAM:  ~2.5 GB
â€¢ CPU:  1-4 cores (depending on load)

Indexing Model Server:
â€¢ Disk: ~2.0 GB
â€¢ RAM:  ~2.5 GB
â€¢ CPU:  1-4 cores (depending on load)

TOTAL FOR BOTH:
â€¢ Disk: ~4.0 GB
â€¢ RAM:  ~5.0 GB
â€¢ CPU:  2-8 cores
```

---

## ğŸ”„ Timeline: First Startup vs Cached Startup

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                       FIRST STARTUP (NO CACHE)                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

00:00 â”‚ Container starts
      â”‚ â””â”€ Check HF_CACHE_PATH
      â”‚
00:10 â”‚ Move temp_huggingface â†’ huggingface cache
      â”‚ â””â”€ Pre-downloaded models from Docker image
      â”‚
00:30 â”‚ Loading nomic-ai/nomic-embed-text-v1
      â”œâ”€ IF in cache: Load from disk (~30s)
      â”‚  â””â”€ âœ… Pre-downloaded in Docker image
      â”‚
      â”œâ”€ IF NOT in cache: Download from Hugging Face (~5-10 min)
      â”‚  â”œâ”€ Download model.safetensors (1.2GB)
      â”‚  â”œâ”€ Download config files
      â”‚  â””â”€ Save to cache
      â”‚
01:30 â”‚ Loading mixedbread-ai/mxbai-rerank-xsmall-v1
      â”œâ”€ IF in cache: Load from disk (~10s)
      â”‚  â””â”€ âœ… Pre-downloaded in Docker image
      â”‚
02:00 â”‚ Loading intent/content classifier
      â”œâ”€ IF in cache: Load from disk (~10s)
      â”‚  â””â”€ âœ… Pre-downloaded in Docker image
      â”‚
02:30 â”‚ Model warmup
      â”œâ”€ Pre-warm RoPE caches
      â””â”€ Test with dummy data
      â”‚
03:00 â”‚ âœ… READY TO SERVE

Total Time: ~3 minutes (with pre-downloaded models in image)
         OR ~10-15 minutes (if downloading from Hugging Face)


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    SUBSEQUENT STARTUPS (WITH CACHE)                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

00:00 â”‚ Container starts
      â”‚ â””â”€ Check HF_CACHE_PATH
      â”‚
00:05 â”‚ Models found in cache! âœ…
      â”‚ â””â”€ /app/.cache/huggingface/models--nomic-ai--nomic-embed-text-v1/
      â”‚
00:10 â”‚ Loading nomic-ai/nomic-embed-text-v1
      â””â”€ Load from cache (no download needed!)
      â”‚
00:40 â”‚ Loading mixedbread-ai/mxbai-rerank-xsmall-v1
      â””â”€ Load from cache
      â”‚
00:50 â”‚ Loading intent/content classifier
      â””â”€ Load from cache
      â”‚
01:00 â”‚ Model warmup
      â””â”€ Pre-warm RoPE caches
      â”‚
01:30 â”‚ âœ… READY TO SERVE

Total Time: ~1.5 minutes âœ…
Internet: NOT REQUIRED âœ…
```

---

## ğŸ¯ Key Takeaways

### âœ… What You Need to Know:

1. **Both model servers use Hugging Face models** - they are essentially Hugging Face model runners
2. **Default embedding model**: `nomic-ai/nomic-embed-text-v1` (1.5GB)
3. **Models are downloaded** from Hugging Face Hub on first use (or pre-downloaded in Docker image)
4. **Models are cached** in `/app/.cache/huggingface/` volume
5. **Same embedding model** used by both servers, different workloads
6. **Inference server**: Real-time queries (single requests)
7. **Indexing server**: Bulk processing (batch requests)
8. **Total size**: ~2-3GB per server, ~4-6GB for both
9. **Startup time**: 1-2 minutes (cached) vs 5-15 minutes (first time with download)
10. **Internet required**: Only for first-time model download

### âš ï¸ Important for Kubernetes:

- **Current setup uses `emptyDir`** - models re-downloaded on every pod restart
- **Recommendation**: Use PersistentVolumeClaims for production
- **Save bandwidth**: Cache models persistently to avoid re-downloading 2-3GB
- **Faster restarts**: Cached models load in 1-2 minutes vs 5-15 minutes

### ğŸ”§ The model servers are:

- **NOT custom AI models** - they use open-source Hugging Face models
- **NOT just API proxies** - they run actual inference (PyTorch/Transformers)
- **Local processing** - embeddings generated on your infrastructure
- **Privacy-focused** - no external API calls after models are downloaded
- **Production-ready** - used by thousands of Onyx deployments

---

**The model servers are the "brain" of Onyx's semantic search, powered by Hugging Face!** ğŸ§ âœ¨

