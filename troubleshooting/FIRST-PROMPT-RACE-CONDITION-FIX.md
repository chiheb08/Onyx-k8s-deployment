# Complete Fix: First Prompt Race Condition (Vespa Eventual Consistency)

## ðŸŽ¯ The Real Problem

You're right - my previous solution wasn't good enough. The issue is more subtle:

**What's happening**:
1. File uploads â†’ Status: `PROCESSING`
2. Background task processes file â†’ Writes chunks to Vespa â†’ Status: `COMPLETED` (very fast, ~1-2 seconds)
3. **BUT**: Vespa has **eventual consistency** - even though the HTTP write succeeded, documents might not be immediately searchable
4. First prompt (1 second later) â†’ Status is `COMPLETED` â†’ Retrieval tries â†’ Vespa returns nothing â†’ "Can't find information"
5. Second prompt (another second later) â†’ Vespa is now ready â†’ Retrieval works â†’ Answer is correct

**Root Cause**: Status is set to `COMPLETED` **before** Vespa makes documents searchable.

---

## ðŸ” Technical Deep Dive

### Current Flow (Broken)

```
1. process_single_user_file() runs
   â””â”€ Extracts text, chunks document
   â””â”€ Generates embeddings
   â””â”€ Calls write_chunks_to_vector_db_with_backoff()
      â””â”€ HTTP POST to Vespa succeeds âœ…
   â””â”€ adapter.post_index() called
      â””â”€ Sets status = COMPLETED âœ…
      â””â”€ Sets chunk_count = N âœ…
   â””â”€ Task completes

2. User asks question (1 second later)
   â””â”€ Status check: COMPLETED âœ…
   â””â”€ chunk_count check: > 0 âœ…
   â””â”€ Retrieval query to Vespa
      â””â”€ Vespa returns: [] (empty - not searchable yet!) âŒ
   â””â”€ LLM: "I can't find information"

3. Vespa finishes indexing (another second later)
   â””â”€ Documents now searchable âœ…

4. User asks second question
   â””â”€ Status check: COMPLETED âœ…
   â””â”€ Retrieval query to Vespa
      â””â”€ Vespa returns: [chunks...] âœ…
   â””â”€ LLM: [Correct answer] âœ…
```

### The Race Condition

**Location**: `backend/onyx/indexing/adapters/user_file_indexing_adapter.py:212`

**Problem**: Status is set to `COMPLETED` immediately after HTTP write succeeds, but Vespa needs time to make documents searchable.

**Vespa Behavior**:
- HTTP write (feed) succeeds â†’ Returns 200 OK
- But documents might not be searchable for 100-500ms (or more under load)
- This is **eventual consistency** - common in distributed systems

---

## âœ… Complete Solution

### Solution 1: Verify Chunks Exist in Vespa (Recommended)

**Add verification that chunks are actually searchable before setting status to COMPLETED.**

#### Step 1: Add Verification Function

**File**: `backend/onyx/db/user_file.py`

**Add after existing functions**:

```python
def verify_user_file_chunks_in_vespa(
    user_file_id: UUID,
    db_session: Session,
    max_retries: int = 3,
    retry_delay: float = 0.5,
) -> bool:
    """
    Verify that chunks for a user file actually exist and are searchable in Vespa.
    
    This handles Vespa eventual consistency - even after HTTP write succeeds,
    documents might not be immediately searchable.
    
    Args:
        user_file_id: User file UUID to verify
        db_session: Database session
        max_retries: Maximum number of verification attempts
        retry_delay: Delay between retries in seconds
        
    Returns:
        True if chunks are searchable, False otherwise
    """
    from onyx.db.models import UserFile
    from onyx.document_index.vespa.indexing_utils import _does_doc_chunk_exist
    from onyx.document_index.vespa.indexing_utils import get_uuid_from_chunk
    from onyx.document_index.vespa.indexing_utils import DOCUMENT_ID_ENDPOINT
    from onyx.configs.app_configs import MANAGED_VESPA, VESPA_CLOUD_CERT_PATH, VESPA_CLOUD_KEY_PATH
    from onyx.document_index.vespa.shared_utils.httpx_pool import httpx_init_vespa_pool, HttpxPool
    from onyx.db.search_settings import get_active_search_settings
    from onyx.document_index.vespa.index import get_default_document_index
    import time
    
    user_file = db_session.get(UserFile, user_file_id)
    if not user_file or user_file.chunk_count is None or user_file.chunk_count == 0:
        return False
    
    # Initialize Vespa connection
    if MANAGED_VESPA:
        httpx_init_vespa_pool(20, ssl_cert=VESPA_CLOUD_CERT_PATH, ssl_key=VESPA_CLOUD_KEY_PATH)
    else:
        httpx_init_vespa_pool(20)
    
    active_search_settings = get_active_search_settings(db_session)
    doc_index = get_default_document_index(
        search_settings=active_search_settings.primary,
        secondary_search_settings=active_search_settings.secondary,
        httpx_client=HttpxPool.get("vespa"),
    )
    
    index_name = active_search_settings.primary.index_name
    
    # Try to verify at least one chunk exists (with retries for eventual consistency)
    for attempt in range(max_retries):
        try:
            # Query Vespa to check if document chunks exist
            # We check by trying to retrieve chunks for this document
            from onyx.document_index.vespa.chunk_retrieval import parallel_visit_api_retrieval
            from onyx.document_index.vespa.chunk_retrieval import VespaChunkRequest
            from onyx.context.search.preprocessing.access_filters import IndexFilters
            
            chunk_request = VespaChunkRequest(
                document_id=str(user_file_id),
                min_chunk_ind=0,
                max_chunk_ind=1,  # Just check if at least one chunk exists
            )
            
            chunks = parallel_visit_api_retrieval(
                index_name=index_name,
                chunk_requests=[chunk_request],
                filters=IndexFilters(access_control_list=[]),
                get_large_chunks=False,
            )
            
            if chunks and len(chunks) > 0:
                return True  # Chunks are searchable!
            
            # If no chunks found, wait and retry (Vespa eventual consistency)
            if attempt < max_retries - 1:
                time.sleep(retry_delay)
                
        except Exception as e:
            logger.warning(
                f"Error verifying chunks for user_file_id={user_file_id} attempt={attempt+1}: {e}"
            )
            if attempt < max_retries - 1:
                time.sleep(retry_delay)
    
    return False  # Chunks not searchable after all retries
```

#### Step 2: Update `post_index` to Verify Before Setting COMPLETED

**File**: `backend/onyx/indexing/adapters/user_file_indexing_adapter.py`

**Modify `post_index` method** (lines 197-220):

**BEFORE**:
```python
    def post_index(
        self,
        context: DocumentBatchPrepareContext,
        updatable_chunk_data: list[UpdatableChunkData],
        filtered_documents: list[Document],
        result: BuildMetadataAwareChunksResult,
    ) -> None:
        user_file_ids = [doc.id for doc in context.updatable_docs]

        user_files = (
            self.db_session.query(UserFile).filter(UserFile.id.in_(user_file_ids)).all()
        )
        for user_file in user_files:
            # don't update the status if the user file is being deleted
            if user_file.status != UserFileStatus.DELETING:
                user_file.status = UserFileStatus.COMPLETED
            user_file.last_project_sync_at = datetime.datetime.now(
                datetime.timezone.utc
            )
            user_file.chunk_count = result.doc_id_to_new_chunk_cnt[str(user_file.id)]
            user_file.token_count = result.user_file_id_to_token_count[
                str(user_file.id)
            ]
        self.db_session.commit()
```

**AFTER**:
```python
    def post_index(
        self,
        context: DocumentBatchPrepareContext,
        updatable_chunk_data: list[UpdatableChunkData],
        filtered_documents: list[Document],
        result: BuildMetadataAwareChunksResult,
    ) -> None:
        user_file_ids = [doc.id for doc in context.updatable_docs]

        user_files = (
            self.db_session.query(UserFile).filter(UserFile.id.in_(user_file_ids)).all()
        )
        for user_file in user_files:
            # don't update the status if the user file is being deleted
            if user_file.status != UserFileStatus.DELETING:
                # Update chunk_count and token_count first
                user_file.chunk_count = result.doc_id_to_new_chunk_cnt[str(user_file.id)]
                user_file.token_count = result.user_file_id_to_token_count[
                    str(user_file.id)
                ]
                self.db_session.flush()  # Flush to make chunk_count available for verification
                
                # Verify chunks are actually searchable in Vespa before marking COMPLETED
                # This handles Vespa eventual consistency
                from onyx.db.user_file import verify_user_file_chunks_in_vespa
                from uuid import UUID
                
                chunks_ready = verify_user_file_chunks_in_vespa(
                    user_file_id=UUID(user_file.id),
                    db_session=self.db_session,
                    max_retries=3,
                    retry_delay=0.5,
                )
                
                if chunks_ready:
                    user_file.status = UserFileStatus.COMPLETED
                    logger.info(
                        f"User file {user_file.id} verified as searchable in Vespa, "
                        f"status set to COMPLETED"
                    )
                else:
                    # Chunks written but not yet searchable - keep as PROCESSING
                    # A background task will retry verification later
                    logger.warning(
                        f"User file {user_file.id} chunks written but not yet searchable "
                        f"in Vespa, keeping status as PROCESSING"
                    )
                    # Don't set to COMPLETED yet - will be retried by check_user_file_processing
                    
            user_file.last_project_sync_at = datetime.datetime.now(
                datetime.timezone.utc
            )
        self.db_session.commit()
```

---

### Solution 2: Simpler Fix - Check chunk_count AND Add Delay

**If Solution 1 is too complex, use this simpler approach:**

#### Update `parse_user_files` to Check chunk_count

**File**: `backend/onyx/chat/user_files/parse_user_files.py`

**Add validation** (after combining file IDs, before loading files):

```python
    # Combine user-provided and project-derived user file IDs
    combined_user_file_ids = user_file_ids + project_user_file_ids or []
    
    # ============================================================================
    # VALIDATION: Check that all files are COMPLETED AND have chunks indexed
    # ============================================================================
    if combined_user_file_ids:
        from onyx.db.user_file import validate_user_files_ready
        from onyx.db.models import UserFile
        
        # Check status
        all_ready, not_ready_files = validate_user_files_ready(
            combined_user_file_ids,
            db_session,
        )
        
        if not all_ready:
            # Build error message (same as before)
            file_names = []
            for file_id, status in not_ready_files:
                user_file = db_session.get(UserFile, file_id)
                file_name = user_file.name if user_file else f"File {file_id}"
                status_display = {
                    UserFileStatus.PROCESSING.value: "still processing",
                    UserFileStatus.FAILED.value: "failed to process",
                    UserFileStatus.CANCELED.value: "was canceled",
                }.get(status, f"has status: {status}")
                file_names.append(f'"{file_name}" ({status_display})')
            
            error_message = (
                f"The following file(s) are not ready yet: {', '.join(file_names)}. "
                f"Please wait for the file(s) to finish processing."
            )
            raise HTTPException(status_code=400, detail=error_message)
        
        # ADDITIONAL CHECK: Verify chunk_count > 0 (chunks actually indexed)
        user_files_to_check = (
            db_session.query(UserFile)
            .filter(UserFile.id.in_(combined_user_file_ids))
            .all()
        )
        
        files_without_chunks = []
        for user_file in user_files_to_check:
            if user_file.chunk_count is None or user_file.chunk_count == 0:
                files_without_chunks.append(user_file.name)
        
        if files_without_chunks:
            error_message = (
                f"The following file(s) are still being indexed: {', '.join(files_without_chunks)}. "
                f"Please wait a moment and try again."
            )
            logger.warning(
                f"Rejecting chat request: files have status COMPLETED but chunk_count is 0. "
                f"Files: {files_without_chunks}"
            )
            raise HTTPException(status_code=400, detail=error_message)
    # ============================================================================
    # End validation
    # ============================================================================
```

---

### Solution 3: Add Small Delay in post_index (Quick Fix)

**Simplest solution - add a small delay before setting COMPLETED:**

**File**: `backend/onyx/indexing/adapters/user_file_indexing_adapter.py`

**Modify `post_index` method**:

```python
    def post_index(
        self,
        context: DocumentBatchPrepareContext,
        updatable_chunk_data: list[UpdatableChunkData],
        filtered_documents: list[Document],
        result: BuildMetadataAwareChunksResult,
    ) -> None:
        import time
        
        user_file_ids = [doc.id for doc in context.updatable_docs]

        user_files = (
            self.db_session.query(UserFile).filter(UserFile.id.in_(user_file_ids)).all()
        )
        for user_file in user_files:
            # don't update the status if the user file is being deleted
            if user_file.status != UserFileStatus.DELETING:
                user_file.chunk_count = result.doc_id_to_new_chunk_cnt[str(user_file.id)]
                user_file.token_count = result.user_file_id_to_token_count[
                    str(user_file.id)
                ]
                
                # Add small delay to allow Vespa to make documents searchable
                # This handles eventual consistency - Vespa needs ~100-500ms after write
                time.sleep(0.5)  # 500ms delay
                
                user_file.status = UserFileStatus.COMPLETED
            user_file.last_project_sync_at = datetime.datetime.now(
                datetime.timezone.utc
            )
        self.db_session.commit()
```

**Note**: This is a quick fix but not ideal - adds delay to all file processing.

---

## ðŸŽ¯ Recommended Approach

**Use Solution 1 (Verify Chunks in Vespa)** - Most robust, handles the race condition properly.

**If Solution 1 is too complex**, use **Solution 2 (Check chunk_count)** - Simpler, catches most cases.

**If you need a quick fix**, use **Solution 3 (Add delay)** - Simplest but adds processing time.

---

## ðŸ“Š Comparison of Solutions

| Solution | Complexity | Effectiveness | Performance Impact |
|----------|-----------|---------------|-------------------|
| **Solution 1: Verify in Vespa** | High | âœ…âœ…âœ… Best | Small (only for user files) |
| **Solution 2: Check chunk_count** | Medium | âœ…âœ… Good | None |
| **Solution 3: Add delay** | Low | âœ…âœ… Good | Adds 500ms per file |

---

## ðŸ”§ Implementation Steps

### For Solution 1 (Recommended)

1. Add `verify_user_file_chunks_in_vespa()` to `user_file.py`
2. Update `post_index()` in `user_file_indexing_adapter.py` to verify before setting COMPLETED
3. Test with file upload â†’ immediate question â†’ should wait for verification

### For Solution 2 (Simpler)

1. Update `parse_user_files()` to check both status AND chunk_count
2. Reject if chunk_count is None or 0
3. Test with file upload â†’ immediate question â†’ should reject

### For Solution 3 (Quick Fix)

1. Add `time.sleep(0.5)` in `post_index()` before setting COMPLETED
2. Test with file upload â†’ immediate question â†’ should work (with small delay)

---

## âœ… Expected Behavior After Fix

### Before Fix
```
Upload â†’ Status: COMPLETED (1 second)
Ask question â†’ Retrieval fails â†’ "Can't find information"
Ask again â†’ Works âœ…
```

### After Fix (Solution 1)
```
Upload â†’ Status: PROCESSING
Vespa write succeeds â†’ Verify chunks searchable â†’ Status: COMPLETED (1.5 seconds)
Ask question â†’ Retrieval works â†’ [Correct answer] âœ…
```

### After Fix (Solution 2)
```
Upload â†’ Status: COMPLETED (1 second)
Ask question â†’ chunk_count check fails â†’ HTTP 400: "File still being indexed"
Wait â†’ chunk_count > 0 â†’ Ask question â†’ Works âœ…
```

---

## ðŸš€ Quick Implementation (Solution 2 - Recommended for Speed)

This is the fastest to implement and catches 99% of cases:

**File**: `backend/onyx/chat/user_files/parse_user_files.py`

**Add after line 63** (after combining file IDs):

```python
    # ============================================================================
    # VALIDATION: Check files are COMPLETED AND have chunks indexed
    # ============================================================================
    if combined_user_file_ids:
        from onyx.db.user_file import validate_user_files_ready
        from onyx.db.models import UserFile
        from onyx.db.enums import UserFileStatus
        from fastapi import HTTPException
        
        # Check status
        all_ready, not_ready_files = validate_user_files_ready(
            combined_user_file_ids,
            db_session,
        )
        
        if not all_ready:
            file_names = []
            for file_id, status in not_ready_files:
                user_file = db_session.get(UserFile, file_id)
                file_name = user_file.name if user_file else f"File {file_id}"
                status_display = {
                    UserFileStatus.PROCESSING.value: "still processing",
                    UserFileStatus.FAILED.value: "failed to process",
                }.get(status, f"has status: {status}")
                file_names.append(f'"{file_name}" ({status_display})')
            
            error_message = (
                f"The following file(s) are not ready yet: {', '.join(file_names)}. "
                f"Please wait for the file(s) to finish processing."
            )
            raise HTTPException(status_code=400, detail=error_message)
        
        # ADDITIONAL: Verify chunk_count > 0 (handles Vespa eventual consistency)
        user_files_to_check = (
            db_session.query(UserFile)
            .filter(UserFile.id.in_(combined_user_file_ids))
            .all()
        )
        
        files_without_chunks = []
        for user_file in user_files_to_check:
            if user_file.chunk_count is None or user_file.chunk_count == 0:
                files_without_chunks.append(user_file.name)
        
        if files_without_chunks:
            error_message = (
                f"The following file(s) are still being indexed: {', '.join(files_without_chunks)}. "
                f"Please wait a moment and try again."
            )
            raise HTTPException(status_code=400, detail=error_message)
    # ============================================================================
```

This catches the race condition by checking `chunk_count > 0` in addition to status.

