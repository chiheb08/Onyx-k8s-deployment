# ============================================================================
# Indexing Model Server for Onyx
# ============================================================================
# Second model server dedicated to document indexing and background processing
# Image: onyxdotapp/onyx-model-server:nightly-20241004
# Port: 9000 (same as inference, but different container)
# Purpose: Bulk document embedding for background workers
# 
# DEPLOYMENT OPTIONS:
# 1. With existing PV (recommended for air-gapped): Use pvc-shared-models.yaml
# 2. With internet access: Use emptyDir (comment out PVC, uncomment emptyDir)
# ============================================================================

---
apiVersion: v1
kind: Service
metadata:
  name: indexing-model-server
  labels:
    app: indexing-model-server
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 9000
      targetPort: 9000
      protocol: TCP
  selector:
    app: indexing-model-server

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: indexing-model-server
  labels:
    app: indexing-model-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: indexing-model-server
  template:
    metadata:
      labels:
        app: indexing-model-server
    spec:
      containers:
        - name: indexing-model-server
          image: onyxdotapp/onyx-model-server:nightly-20241004
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 9000
              protocol: TCP
          command:
            - uvicorn
            - model_server.main:app
            - --host
            - "0.0.0.0"
            - --port
            - "9000"
            - --limit-concurrency
            - "4"  # Limits concurrent requests for indexing (as per Helm)
          envFrom:
            - configMapRef:
                name: onyx-config
          env:
            # ================================================================
            # CRITICAL: INDEXING_ONLY=True makes this the indexing server
            # ================================================================
            - name: INDEXING_ONLY
              value: "True"
            # ================================================================
            # OFFLINE MODE (for air-gapped/restricted environments)
            # Uncomment these if using existing PV with pre-loaded models
            # ================================================================
            - name: HF_HOME
              value: "/app/.cache/huggingface"
            - name: HF_HUB_OFFLINE
              value: "1"  # Force offline - don't download from Hugging Face
            - name: TRANSFORMERS_OFFLINE
              value: "1"  # Force transformers to use cached models only
            - name: MODEL_SERVER_PORT
              value: "9000"
            - name: INDEXING_MODEL_SERVER_PORT
              value: "9000"
          resources:
            requests:
              cpu: 1000m      # Higher CPU for bulk processing
              memory: 2Gi     # Same memory as inference server
            limits:
              cpu: 4000m      # Higher limits for heavy indexing
              memory: 8Gi     # Same memory as inference server
          livenessProbe:
            httpGet:
              path: /health
              port: 9000
            initialDelaySeconds: 120
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 5
          readinessProbe:
            httpGet:
              path: /health
              port: 9000
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 10
          volumeMounts:
            - name: indexing-model-cache
              mountPath: /app/.cache/huggingface
              readOnly: true  # Read-only since models are pre-loaded
      volumes:
        # ================================================================
        # OPTION 1: Use existing PV with pre-loaded models (RECOMMENDED)
        # For air-gapped/restricted environments
        # Shares the SAME PVC as inference server (if ReadWriteMany)
        # Requires: pvc-shared-models.yaml to be deployed first
        # ================================================================
        - name: indexing-model-cache
          persistentVolumeClaim:
            claimName: huggingface-models-pvc
        
        # ================================================================
        # OPTION 2: Use emptyDir (for environments with internet access)
        # Uncomment below and comment out PVC above if you have internet
        # Models will download on first start (~2-3GB, 5-10 minutes)
        # ================================================================
        # - name: indexing-model-cache
        #   emptyDir: {}
      restartPolicy: Always
