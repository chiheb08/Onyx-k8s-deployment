# ============================================================================
# Inference Model Server Deployment for Onyx
# ============================================================================
# AI/ML embeddings server for real-time query processing
# Image: onyxdotapp/onyx-model-server:nightly-20241004
# Port: 9000
# 
# DEPLOYMENT OPTIONS:
# 1. With existing PV (recommended for air-gapped): Use pvc-shared-models.yaml
# 2. With internet access: Use emptyDir (comment out PVC, uncomment emptyDir)
# ============================================================================

---
apiVersion: v1
kind: Service
metadata:
  name: inference-model-server
  labels:
    app: inference-model-server
spec:
  type: ClusterIP
  ports:
    - name: modelserver
      port: 9000
      targetPort: 9000
      protocol: TCP
  selector:
    app: inference-model-server

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: inference-model-server
  labels:
    app: inference-model-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: inference-model-server
  template:
    metadata:
      labels:
        app: inference-model-server
    spec:
      containers:
        - name: model-server
          image: onyxdotapp/onyx-model-server:nightly-20241004
          imagePullPolicy: IfNotPresent
          ports:
            - name: modelserver
              containerPort: 9000
              protocol: TCP
          command:
            - uvicorn
            - model_server.main:app
            - --host
            - "0.0.0.0"
            - --port
            - "9000"
          envFrom:
            - configMapRef:
                name: onyx-config
          env:
            # ================================================================
            # OFFLINE MODE (for air-gapped/restricted environments)
            # Uncomment these if using existing PV with pre-loaded models
            # ================================================================
            - name: HF_HOME
              value: "/app/.cache/huggingface"
            - name: HF_HUB_OFFLINE
              value: "1"  # Force offline - don't download from Hugging Face
            - name: TRANSFORMERS_OFFLINE
              value: "1"  # Force transformers to use cached models only
            - name: MODEL_SERVER_PORT
              value: "9000"
            - name: INDEXING_MODEL_SERVER_PORT
              value: "9000"
          resources:
            requests:
              cpu: 500m          # Reduced from Helm (2000m) for minimal
              memory: 2Gi        # Reduced from Helm (6Gi) for minimal
            limits:
              cpu: 2000m         # Reduced from Helm (4000m)
              memory: 4Gi        # Reduced from Helm (10Gi)
          livenessProbe:
            httpGet:
              path: /health
              port: 9000
            initialDelaySeconds: 120
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /health
              port: 9000
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 5
          volumeMounts:
            - name: model-cache
              mountPath: /app/.cache/huggingface
              readOnly: true  # Read-only since models are pre-loaded
      volumes:
        # ================================================================
        # OPTION 1: Use existing PV with pre-loaded models (RECOMMENDED)
        # For air-gapped/restricted environments
        # Requires: pvc-shared-models.yaml to be deployed first
        # ================================================================
        - name: model-cache
          persistentVolumeClaim:
            claimName: huggingface-models-pvc
        
        # ================================================================
        # OPTION 2: Use emptyDir (for environments with internet access)
        # Uncomment below and comment out PVC above if you have internet
        # Models will download on first start (~2-3GB, 5-10 minutes)
        # ================================================================
        # - name: model-cache
        #   emptyDir: {}
      restartPolicy: Always

