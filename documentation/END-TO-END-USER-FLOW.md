# End-to-End User Flow - Complete Architecture

**Complete journey from user opening browser to receiving an AI-powered answer from LLM**

---

## ğŸ¯ The Complete Picture

This document shows **EVERY STEP** of how Onyx works, from the moment a user opens their browser until they see an AI-generated answer.

---

## ğŸ—ï¸ Complete Architecture Diagram

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    COMPLETE ONYX ARCHITECTURE                             â•‘
â•‘                    From Browser to LLM Response                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ‘¤ USER                                                                 â”‚
â”‚  Opens browser: http://nginx-onyx-infra.apps.cluster.company.com       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ HTTP Request
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸŒ OPENSHIFT ROUTE                                                      â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                  â”‚
â”‚  Name: nginx                                                             â”‚
â”‚  Host: nginx-onyx-infra.apps.cluster.company.com                        â”‚
â”‚  Target Service: nginx:80                                                â”‚
â”‚  Type: edge (HTTP/HTTPS)                                                 â”‚
â”‚                                                                          â”‚
â”‚  What it does: External URL â†’ Internal Service                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ Routes to nginx Service
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“¡ NGINX SERVICE                                                        â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                        â”‚
â”‚  Name: nginx                                                             â”‚
â”‚  Type: ClusterIP                                                         â”‚
â”‚  Port: 80                                                                â”‚
â”‚  Selector: app=nginx                                                     â”‚
â”‚                                                                          â”‚
â”‚  What it does: Load balances to nginx pods                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ Routes to nginx pod
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“¡ NGINX POD (Reverse Proxy)                                           â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                           â”‚
â”‚  Container: nginx:1.23.4-alpine                                          â”‚
â”‚  Port: 80                                                                â”‚
â”‚  ConfigMap: nginx-config (routing rules)                                 â”‚
â”‚                                                                          â”‚
â”‚  ROUTING RULES (from ConfigMap):                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚                                                            â”‚         â”‚
â”‚  â”‚  location / {                                              â”‚         â”‚
â”‚  â”‚      proxy_pass http://web-server:3000;                   â”‚         â”‚
â”‚  â”‚  }                                                         â”‚         â”‚
â”‚  â”‚  â†’ All requests for / go to Web Server                    â”‚         â”‚
â”‚  â”‚                                                            â”‚         â”‚
â”‚  â”‚  location /api/ {                                          â”‚         â”‚
â”‚  â”‚      proxy_pass http://api-server:8080;                   â”‚         â”‚
â”‚  â”‚  }                                                         â”‚         â”‚
â”‚  â”‚  â†’ All requests for /api/* go to API Server               â”‚         â”‚
â”‚  â”‚                                                            â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                                          â”‚
â”‚  Decision: Is URL path /api/* ?                                         â”‚
â”‚  â”œâ”€ NO  â†’ Send to Web Server (Next.js)                                  â”‚
â”‚  â””â”€ YES â†’ Send to API Server (FastAPI)                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                               â”‚
                    â”‚ /                             â”‚ /api/*
                    â†“                               â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ğŸ–¥ï¸  WEB SERVER           â”‚   â”‚  âš™ï¸  API SERVER                   â”‚
    â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•           â”‚   â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                  â”‚
    â”‚  Service: web-server      â”‚   â”‚  Service: api-server              â”‚
    â”‚  Port: 3000               â”‚   â”‚  Port: 8080                       â”‚
    â”‚  Image: onyx-web-server   â”‚   â”‚  Image: onyx-backend              â”‚
    â”‚                           â”‚   â”‚                                   â”‚
    â”‚  What it serves:          â”‚   â”‚  What it handles:                 â”‚
    â”‚  â€¢ HTML pages             â”‚   â”‚  â€¢ REST API endpoints             â”‚
    â”‚  â€¢ JavaScript/CSS         â”‚   â”‚  â€¢ Authentication                 â”‚
    â”‚  â€¢ Next.js components     â”‚   â”‚  â€¢ Database queries               â”‚
    â”‚  â€¢ Static assets          â”‚   â”‚  â€¢ LLM orchestration              â”‚
    â”‚                           â”‚   â”‚  â€¢ Document search                â”‚
    â”‚  Examples:                â”‚   â”‚                                   â”‚
    â”‚  â€¢ GET /                  â”‚   â”‚  Examples:                        â”‚
    â”‚  â€¢ GET /chat              â”‚   â”‚  â€¢ POST /api/chat                 â”‚
    â”‚  â€¢ GET /search            â”‚   â”‚  â€¢ POST /api/query/stream-answer  â”‚
    â”‚  â€¢ GET /admin             â”‚   â”‚  â€¢ GET /api/user                  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                             â”‚
                â”‚ Calls API for data          â”‚
                â”‚ (AJAX/Fetch requests)       â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ All API calls from browser go through NGINX
                              â”‚ Browser: fetch('/api/chat', ...)
                              â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    API SERVER       â”‚
                    â”‚    Processes:       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                 â”‚                 â”‚
            â†“                 â†“                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ’¾ PostgreSQL  â”‚ â”‚  âš¡ Redis       â”‚ â”‚  ğŸ” Vespa           â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•  â”‚ â”‚  â•â•â•â•â•â•â•â•â•â•â•    â”‚ â”‚  â•â•â•â•â•â•â•â•â•â•â•        â”‚
â”‚  Service:       â”‚ â”‚  Service:       â”‚ â”‚  Service:           â”‚
â”‚  postgresql:5432â”‚ â”‚  redis:6379     â”‚ â”‚  vespa:8081,19071   â”‚
â”‚                 â”‚ â”‚                 â”‚ â”‚                     â”‚
â”‚  Stores:        â”‚ â”‚  Stores:        â”‚ â”‚  Stores:            â”‚
â”‚  â€¢ User data    â”‚ â”‚  â€¢ Sessions     â”‚ â”‚  â€¢ Doc chunks       â”‚
â”‚  â€¢ Doc metadata â”‚ â”‚  â€¢ Cache        â”‚ â”‚  â€¢ Embeddings       â”‚
â”‚  â€¢ Chat history â”‚ â”‚  â€¢ Task queue   â”‚ â”‚  â€¢ Vector index     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                    â”‚
                                                    â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  ğŸ¤– INFERENCE MODEL SERVER  â”‚
                    â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•    â”‚
                    â”‚  Service: inference-model-  â”‚
                    â”‚           server:9000       â”‚
                    â”‚                             â”‚
                    â”‚  Purpose:                   â”‚
                    â”‚  â€¢ Embed user queries       â”‚
                    â”‚  â€¢ Convert text â†’ vectors   â”‚
                    â”‚  â€¢ Enable semantic search   â”‚
                    â”‚                             â”‚
                    â”‚  Models from NFS PVC:       â”‚
                    â”‚  â€¢ nomic-embed-text-v1      â”‚
                    â”‚  â€¢ Offline mode (no internet)â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ Returns embedding vector
                              â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    API SERVER       â”‚
                    â”‚    Uses embedding   â”‚
                    â”‚    to search Vespa  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ Retrieves relevant doc chunks
                              â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  ğŸ§  EXTERNAL LLM            â”‚
                    â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•            â”‚
                    â”‚  (vLLM in another namespace)â”‚
                    â”‚  or OpenAI, Anthropic, etc. â”‚
                    â”‚                             â”‚
                    â”‚  API Server calls:          â”‚
                    â”‚  POST /v1/chat/completions  â”‚
                    â”‚  {                          â”‚
                    â”‚    "messages": [            â”‚
                    â”‚      {"role": "system",...},â”‚
                    â”‚      {"role": "user",       â”‚
                    â”‚       "content": "Context:  â”‚
                    â”‚         <chunks> Question..."â”‚}
                    â”‚    ]                        â”‚
                    â”‚  }                          â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ Returns AI answer
                              â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    API SERVER       â”‚
                    â”‚    Returns response â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ HTTP Response
                              â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     NGINX           â”‚
                    â”‚     Forwards back   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Web Server        â”‚
                    â”‚   (Next.js)         â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   User's Browser    â”‚
                    â”‚   Displays answer!  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Step-by-Step User Journey

### STEP 1: User Opens Browser

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: USER OPENS ONYX                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User Action:
â”â”â”â”â”â”â”â”â”â”â”
Opens browser and types:
http://nginx-onyx-infra.apps.cluster.company.com

What Happens:
â”â”â”â”â”â”â”â”â”â”â”â”â”
1. DNS resolves the hostname
   â†’ Gets OpenShift router IP
   
2. Browser sends HTTP GET request:
   GET / HTTP/1.1
   Host: nginx-onyx-infra.apps.cluster.company.com
   
3. Request hits OpenShift Router
   â†’ Checks route table
   â†’ Finds route "nginx" matches this hostname
   â†’ Forwards to nginx Service (port 80)
   
4. nginx Service receives request
   â†’ Load balances to nginx Pod
   
5. nginx Pod receives request
   â†’ Path is "/"
   â†’ nginx.conf says: location / â†’ proxy_pass http://web-server:3000
   â†’ Forwards to web-server Service
   
6. web-server Service receives request
   â†’ Forwards to web-server Pod
   
7. web-server Pod (Next.js) processes:
   â†’ Server-side renders the React app
   â†’ Generates HTML with JavaScript
   â†’ Returns HTML page
   
8. HTML flows back:
   web-server Pod â†’ web-server Service â†’ nginx Pod â†’ nginx Service
   â†’ OpenShift Route â†’ Browser
   
9. User sees: Onyx login page! ğŸ‰

Time: ~200-500ms
```

---

### STEP 2: User Logs In

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: USER AUTHENTICATION                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User Action:
â”â”â”â”â”â”â”â”â”â”â”
Enters credentials:
â€¢ Email: user@company.com
â€¢ Password: ********
â€¢ Clicks "Login"

What Happens:
â”â”â”â”â”â”â”â”â”â”â”â”â”
1. Browser JavaScript sends AJAX request:
   POST /api/auth/login HTTP/1.1
   Host: nginx-onyx-infra.apps.cluster.company.com
   Content-Type: application/json
   
   Body: {
     "email": "user@company.com",
     "password": "********"
   }

2. Request flows through NGINX:
   â†’ Path is "/api/auth/login" (starts with /api/)
   â†’ nginx.conf: location /api/ â†’ proxy_pass http://api-server:8080
   â†’ Forwards to api-server Service
   
3. api-server Service â†’ api-server Pod
   
4. API Server (FastAPI) processes login:
   a. Receives POST /api/auth/login
   b. Validates credentials against PostgreSQL
      â†’ Queries: postgresql:5432
      â†’ SELECT * FROM users WHERE email = 'user@company.com'
   c. Checks password hash
   d. Generates session token
   e. Stores session in Redis
      â†’ SET session:abc123xyz "user_id:42"
   f. Returns session cookie
   
5. Response flows back:
   API Server â†’ NGINX â†’ Browser
   
6. Browser stores session cookie
   
7. User sees: Onyx dashboard! ğŸ‰

Time: ~100-300ms
```

---

### STEP 3: User Asks a Question

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3: USER SUBMITS CHAT QUERY                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User Action:
â”â”â”â”â”â”â”â”â”â”â”
Types in chat: "What is our vacation policy?"
Clicks "Send"

What Happens:
â”â”â”â”â”â”â”â”â”â”â”â”â”
1. Browser JavaScript sends:
   POST /api/chat/send-message HTTP/1.1
   Cookie: session=abc123xyz
   
   Body: {
     "message": "What is our vacation policy?",
     "chat_session_id": null,
     "persona_id": 1
   }

2. Request â†’ OpenShift Route â†’ nginx Service â†’ nginx Pod
   
3. NGINX routing decision:
   Path: /api/chat/send-message
   â†’ Starts with /api/
   â†’ nginx.conf: proxy_pass http://api-server:8080
   â†’ Forwards to api-server:8080/api/chat/send-message
   
4. api-server Service â†’ api-server Pod
   
5. API Server (FastAPI) processes:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  API Server Internal Processing                          â”‚
   â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                  â”‚
   â”‚                                                           â”‚
   â”‚  Step 5a: Authentication                                  â”‚
   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                 â”‚
   â”‚  â€¢ Checks session cookie                                  â”‚
   â”‚  â€¢ Queries Redis: GET session:abc123xyz                  â”‚
   â”‚  â€¢ Gets user_id: 42                                       â”‚
   â”‚  â€¢ User authenticated âœ…                                  â”‚
   â”‚                                                           â”‚
   â”‚  Step 5b: Create/Get Chat Session                        â”‚
   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                           â”‚
   â”‚  â€¢ Queries PostgreSQL: INSERT INTO chat_sessions          â”‚
   â”‚  â€¢ Creates new chat session                               â”‚
   â”‚  â€¢ chat_session_id: 789                                   â”‚
   â”‚                                                           â”‚
   â”‚  Step 5c: Save User Message                              â”‚
   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                 â”‚
   â”‚  â€¢ Queries PostgreSQL: INSERT INTO messages               â”‚
   â”‚  â€¢ Saves: "What is our vacation policy?"                  â”‚
   â”‚  â€¢ message_id: 1234                                       â”‚
   â”‚                                                           â”‚
   â”‚  Step 5d: Generate Query Embedding                       â”‚
   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                              â”‚
   â”‚  â€¢ Calls Inference Model Server:                          â”‚
   â”‚    POST http://inference-model-server:9000/embed          â”‚
   â”‚    Body: {                                                â”‚
   â”‚      "texts": ["What is our vacation policy?"]           â”‚
   â”‚    }                                                      â”‚
   â”‚                                                           â”‚
   â”‚  â€¢ inference-model-server Pod receives request            â”‚
   â”‚  â€¢ Loads model from NFS-mounted PVC:                      â”‚
   â”‚    /app/.cache/huggingface/models--nomic-ai--nomic-...   â”‚
   â”‚  â€¢ Converts text to 768-dimensional vector:               â”‚
   â”‚    [0.123, -0.456, 0.789, ..., 0.321]                    â”‚
   â”‚  â€¢ Returns embedding (~100ms)                             â”‚
   â”‚                                                           â”‚
   â”‚  Step 5e: Search Vespa for Relevant Documents            â”‚
   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€               â”‚
   â”‚  â€¢ API Server receives embedding from model server        â”‚
   â”‚  â€¢ Calls Vespa:                                           â”‚
   â”‚    POST http://vespa:8081/search/                         â”‚
   â”‚    Body: {                                                â”‚
   â”‚      "yql": "select * from docs...",                      â”‚
   â”‚      "ranking.features.query(embedding)": [0.123, ...]   â”‚
   â”‚    }                                                      â”‚
   â”‚                                                           â”‚
   â”‚  â€¢ Vespa Pod searches vector index                        â”‚
   â”‚  â€¢ Finds top 10 similar document chunks:                  â”‚
   â”‚    [                                                      â”‚
   â”‚      {text: "Employees receive 15 days...", score: 0.95},â”‚
   â”‚      {text: "Vacation must be approved...", score: 0.87},â”‚
   â”‚      {text: "PTO accrues monthly...", score: 0.82},      â”‚
   â”‚      ...                                                  â”‚
   â”‚    ]                                                      â”‚
   â”‚  â€¢ Returns results (~50ms)                                â”‚
   â”‚                                                           â”‚
   â”‚  Step 5f: Build Context from Chunks                      â”‚
   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                         â”‚
   â”‚  â€¢ API Server receives chunks from Vespa                  â”‚
   â”‚  â€¢ Combines into context string:                          â”‚
   â”‚    "Context: Employees receive 15 days... Vacation must..."â”‚
   â”‚                                                           â”‚
   â”‚  Step 5g: Query Metadata from PostgreSQL                 â”‚
   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                    â”‚
   â”‚  â€¢ Gets document sources/citations                        â”‚
   â”‚  â€¢ Queries: SELECT * FROM documents WHERE id IN (...)    â”‚
   â”‚                                                           â”‚
   â”‚  Step 5h: Call External LLM                              â”‚
   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                  â”‚
   â”‚  â€¢ Prepares LLM request with context + question           â”‚
   â”‚  â€¢ Calls external LLM (vLLM in another namespace)         â”‚
   â”‚    POST http://vllm-service.vllm-namespace:8000/v1/chat/completionsâ”‚
   â”‚    Body: {                                                â”‚
   â”‚      "model": "meta-llama/Meta-Llama-3-8B-Instruct",     â”‚
   â”‚      "messages": [                                        â”‚
   â”‚        {                                                  â”‚
   â”‚          "role": "system",                                â”‚
   â”‚          "content": "You are a helpful assistant..."     â”‚
   â”‚        },                                                 â”‚
   â”‚        {                                                  â”‚
   â”‚          "role": "user",                                  â”‚
   â”‚          "content": "Context:\n---\nEmployees receive... â”‚
   â”‚                      \n---\nQuestion: What is our       â”‚
   â”‚                      vacation policy?"                    â”‚
   â”‚        }                                                  â”‚
   â”‚      ],                                                   â”‚
   â”‚      "max_tokens": 500,                                   â”‚
   â”‚      "temperature": 0.7                                   â”‚
   â”‚    }                                                      â”‚
   â”‚                                                           â”‚
   â”‚  â€¢ vLLM Pod (in vllm-namespace) processes:               â”‚
   â”‚    - Receives request                                     â”‚
   â”‚    - Loads Llama model into GPU/CPU                       â”‚
   â”‚    - Generates answer token-by-token                      â”‚
   â”‚    - Returns: "Based on your company policy..."          â”‚
   â”‚  â€¢ Time: ~2-10 seconds (depending on model/hardware)     â”‚
   â”‚                                                           â”‚
   â”‚  Step 5i: Save AI Response                               â”‚
   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                  â”‚
   â”‚  â€¢ API Server receives LLM response                       â”‚
   â”‚  â€¢ Saves to PostgreSQL: INSERT INTO messages              â”‚
   â”‚  â€¢ Saves to Redis cache (for 5 minutes)                   â”‚
   â”‚                                                           â”‚
   â”‚  Step 5j: Return Response to User                        â”‚
   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                           â”‚
   â”‚  â€¢ Formats response with citations                        â”‚
   â”‚  â€¢ Returns JSON to browser                                â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   
6. Response flows back:
   API Server â†’ nginx Pod â†’ nginx Service â†’ OpenShift Route â†’ Browser
   
7. Browser JavaScript receives response:
   {
     "message": "Based on your company policy, employees receive 15 days...",
     "citations": [...],
     "chat_session_id": 789
   }
   
8. React app updates UI:
   â€¢ Displays AI answer
   â€¢ Shows citations/sources
   â€¢ Updates chat history
   
9. User sees: AI-generated answer! ğŸ‰

Total Time: ~3-12 seconds
```

---

## ğŸŒ Network Routes & Connections Summary

### All HTTP Routes Through NGINX

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              NGINX ROUTING RULES (Complete List)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Route 1: Homepage & Static Pages
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Request: GET /
Routing: NGINX â†’ web-server:3000
Purpose: Serve Next.js UI
Examples:
â€¢ GET / â†’ Login page
â€¢ GET /chat â†’ Chat interface
â€¢ GET /search â†’ Search page
â€¢ GET /admin â†’ Admin panel


Route 2: API Endpoints
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Request: POST/GET /api/*
Routing: NGINX â†’ api-server:8080
Purpose: Backend API calls
Examples:
â€¢ POST /api/auth/login â†’ User login
â€¢ POST /api/chat/send-message â†’ Send chat message
â€¢ POST /api/query/stream-answer â†’ Search with AI
â€¢ GET /api/user â†’ Get user info
â€¢ POST /api/connector/create â†’ Create data connector


Route 3: WebSocket Streaming
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Request: WS /api/stream
Routing: NGINX â†’ api-server:8080 (upgraded to WebSocket)
Purpose: Real-time streaming responses
Example:
â€¢ WS /api/stream â†’ Stream LLM response token-by-token


Route 4: Health Check
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Request: GET /nginx-health
Routing: NGINX (responds directly)
Purpose: NGINX liveness check
Returns: 200 OK "healthy"
```

---

### Internal Service-to-Service Connections

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           INTERNAL SERVICE COMMUNICATION (DNS Names)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

From Web Server:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â€¢ To API Server: http://api-server:8080
  (Next.js server-side calls API for data)


From API Server:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â€¢ To PostgreSQL: postgresql:5432
  Protocol: PostgreSQL wire protocol
  Purpose: Store/retrieve data
  
â€¢ To Redis: redis:6379
  Protocol: Redis protocol
  Purpose: Cache, sessions, task queue
  
â€¢ To Vespa: vespa:8081 (query) / vespa:19071 (feed)
  Protocol: HTTP/gRPC
  Purpose: Vector search, document retrieval
  
â€¢ To Inference Model Server: http://inference-model-server:9000
  Protocol: HTTP (REST API)
  Purpose: Generate query embeddings
  Endpoint: POST /embed
  
â€¢ To External LLM: http://vllm-service.vllm-namespace:8000
  Protocol: HTTP (OpenAI-compatible API)
  Purpose: Generate AI answers
  Endpoint: POST /v1/chat/completions
  
â€¢ To Indexing Model Server: http://indexing-model-server:9000
  Protocol: HTTP (REST API)
  Purpose: Generate document embeddings (if background workers deployed)


Kubernetes DNS Resolution:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Service Name: postgresql
Full DNS: postgresql.onyx-infra.svc.cluster.local
Short Name: postgresql (works within same namespace)
Resolves to: ClusterIP (e.g., 10.96.123.45)

Service Name: inference-model-server
Full DNS: inference-model-server.onyx-infra.svc.cluster.local
Short Name: inference-model-server (works within same namespace)
Resolves to: ClusterIP (e.g., 10.96.123.50)

Cross-Namespace (for vLLM):
Service Name: vllm-service.vllm-namespace
Full DNS: vllm-service.vllm-namespace.svc.cluster.local
Must use: Namespace qualifier (vllm-namespace)
```

---

## ğŸ”„ Complete Request Flow - Detailed

### Example: "What is our vacation policy?"

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              COMPLETE REQUEST FLOW (Every Network Hop)                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


TIME  â”‚  COMPONENT                    â”‚  ACTION
â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
00:00 â”‚  User Browser                 â”‚  User types question, clicks Send
      â”‚  Location: User's laptop      â”‚
      â†“
00:01 â”‚  Browser JavaScript           â”‚  Sends AJAX request:
      â”‚  (React app)                  â”‚  POST /api/chat/send-message
      â”‚                               â”‚  Headers: Cookie, Content-Type
      â”‚                               â”‚  Body: {"message": "What is..."}
      â†“
00:02 â”‚  DNS Resolution               â”‚  Resolves:
      â”‚                               â”‚  nginx-onyx-infra.apps.cluster...
      â”‚                               â”‚  â†’ OpenShift Router IP
      â†“
00:03 â”‚  OpenShift Router             â”‚  Receives HTTP request
      â”‚  (HAProxy/Ingress)            â”‚  Checks route table
      â”‚                               â”‚  Finds: route "nginx" matches hostname
      â”‚                               â”‚  Forwards to: nginx Service
      â†“
00:04 â”‚  Service: nginx               â”‚  ClusterIP: 10.96.100.10
      â”‚  Port: 80                     â”‚  Selects pod: nginx-abc123
      â”‚                               â”‚  Forwards to: nginx Pod port 80
      â†“
00:05 â”‚  Pod: nginx-abc123            â”‚  NGINX receives request
      â”‚  Container: nginx             â”‚  Path: /api/chat/send-message
      â”‚  IP: 10.244.0.50              â”‚  Checks nginx.conf routing:
      â”‚                               â”‚  location /api/ â†’ upstream api_server
      â”‚                               â”‚  Proxies to: http://api-server:8080
      â†“
00:06 â”‚  DNS: api-server              â”‚  Resolves to: 10.96.100.20
      â†“
00:07 â”‚  Service: api-server          â”‚  ClusterIP: 10.96.100.20
      â”‚  Port: 8080                   â”‚  Selects pod: api-server-xyz789
      â”‚                               â”‚  Forwards to: api-server Pod
      â†“
00:08 â”‚  Pod: api-server-xyz789       â”‚  FastAPI receives request
      â”‚  Container: onyx-backend      â”‚  Endpoint: /api/chat/send-message
      â”‚  IP: 10.244.0.51              â”‚  Handler: chat_router.send_message()
      â†“
00:09 â”‚  API â†’ PostgreSQL             â”‚  Connection: postgresql:5432
      â”‚                               â”‚  Query: INSERT INTO messages
      â”‚                               â”‚  Query: SELECT user permissions
      â”‚                               â”‚  Response: 20ms
      â†“
00:10 â”‚  API â†’ Redis                  â”‚  Connection: redis:6379
      â”‚                               â”‚  Check cache: GET query:hash
      â”‚                               â”‚  Cache MISS
      â”‚                               â”‚  Response: 5ms
      â†“
00:11 â”‚  API â†’ Inference Model Server â”‚  POST http://inference-model-server:9000/embed
      â”‚                               â”‚  Body: {"texts": ["What is our vacation..."]}
      â†“
00:12 â”‚  DNS: inference-model-server  â”‚  Resolves to: 10.96.100.30
      â†“
00:13 â”‚  Service: inference-model-    â”‚  ClusterIP: 10.96.100.30
      â”‚  server, Port: 9000           â”‚  Forwards to: inference-model-server Pod
      â†“
00:14 â”‚  Pod: inference-model-        â”‚  FastAPI receives /embed request
      â”‚  server-mmm999                â”‚  Loads model from:
      â”‚  IP: 10.244.0.52              â”‚  /app/.cache/huggingface/ (NFS PVC!)
      â”‚                               â”‚  
      â”‚  Model Server Processing:     â”‚  
      â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚  
      â”‚  â€¢ Loads nomic-embed-text-v1  â”‚  (from NFS)
      â”‚  â€¢ Tokenizes text             â”‚
      â”‚  â€¢ Runs neural network        â”‚
      â”‚  â€¢ Generates 768-dim vector   â”‚
      â”‚  â€¢ Returns: [0.123, -0.456, ...]  (~100ms)
      â†“
00:15 â”‚  API Server (back)            â”‚  Receives embedding vector
      â”‚                               â”‚  Now searches Vespa
      â†“
00:16 â”‚  API â†’ Vespa                  â”‚  POST http://vespa:8081/search/
      â”‚                               â”‚  Body: vector + query parameters
      â†“
00:17 â”‚  DNS: vespa                   â”‚  Resolves to: vespa-0.vespa (StatefulSet)
      â”‚                               â”‚  IP: 10.244.0.53
      â†“
00:18 â”‚  Service: vespa               â”‚  Headless service
      â”‚  Port: 8081                   â”‚  Routes to: vespa-0 Pod
      â†“
00:19 â”‚  Pod: vespa-0                 â”‚  Vespa receives search request
      â”‚  Container: vespaengine       â”‚  
      â”‚  IP: 10.244.0.53              â”‚  Vespa Processing:
      â”‚                               â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      â”‚                               â”‚  â€¢ Vector similarity search
      â”‚                               â”‚  â€¢ Compares query embedding with
      â”‚                               â”‚    stored document embeddings
      â”‚                               â”‚  â€¢ Ranks by similarity score
      â”‚                               â”‚  â€¢ Returns top 10 chunks
      â”‚                               â”‚  Response: ~50ms
      â†“
00:20 â”‚  API Server (back)            â”‚  Receives 10 document chunks
      â”‚                               â”‚  Builds prompt for LLM:
      â”‚                               â”‚  
      â”‚                               â”‚  System: "You are helpful..."
      â”‚                               â”‚  Context: "<chunk1><chunk2>..."
      â”‚                               â”‚  Question: "What is our vacation..."
      â†“
00:21 â”‚  API â†’ External LLM           â”‚  POST http://vllm-service.vllm-namespace:8000
      â”‚  (vLLM in another namespace)  â”‚       /v1/chat/completions
      â”‚                               â”‚  
      â”‚                               â”‚  Body: {
      â”‚                               â”‚    "model": "llama3...",
      â”‚                               â”‚    "messages": [system, user],
      â”‚                               â”‚    "max_tokens": 500
      â”‚                               â”‚  }
      â†“
00:22 â”‚  DNS: vllm-service.vllm-      â”‚  Cross-namespace DNS resolution
      â”‚  namespace                    â”‚  Resolves to: 10.96.200.50
      â†“
00:23 â”‚  Service: vllm-service        â”‚  ClusterIP: 10.96.200.50
      â”‚  (in vllm-namespace)          â”‚  Port: 8000
      â”‚  Port: 8000                   â”‚  Forwards to: vllm Pod
      â†“
00:24 â”‚  Pod: vllm-server-xxx         â”‚  vLLM receives request
      â”‚  (in vllm-namespace)          â”‚  
      â”‚  Container: vllm/vllm-openai  â”‚  LLM Processing:
      â”‚  IP: 10.244.1.100             â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      â”‚                               â”‚  â€¢ Receives context + question
      â”‚                               â”‚  â€¢ Loads Llama-3-8B model (GPU)
      â”‚                               â”‚  â€¢ Generates tokens:
      â”‚                               â”‚    "Based" "on" "your" "company"...
      â”‚                               â”‚  â€¢ Returns complete answer
      â”‚                               â”‚  Response: ~2-10 seconds
      â†“
00:34 â”‚  API Server (back)            â”‚  Receives LLM response:
      â”‚                               â”‚  {
      â”‚                               â”‚    "choices": [{
      â”‚                               â”‚      "message": {
      â”‚                               â”‚        "content": "Based on your
      â”‚                               â”‚         company policy, employees
      â”‚                               â”‚         receive 15 days of vacation..."
      â”‚                               â”‚      }
      â”‚                               â”‚    }]
      â”‚                               â”‚  }
      â†“
00:35 â”‚  API â†’ PostgreSQL             â”‚  INSERT INTO messages (AI response)
      â”‚                               â”‚  Response: 20ms
      â†“
00:36 â”‚  API â†’ Redis                  â”‚  SET query:hash response (cache)
      â”‚                               â”‚  Response: 5ms
      â†“
00:37 â”‚  API Server                   â”‚  Formats final response with:
      â”‚                               â”‚  â€¢ AI answer
      â”‚                               â”‚  â€¢ Citations/sources
      â”‚                               â”‚  â€¢ Chat session ID
      â”‚                               â”‚  â€¢ Message IDs
      â”‚                               â”‚  Returns JSON to nginx
      â†“
00:38 â”‚  nginx Pod                    â”‚  Receives response from API
      â”‚                               â”‚  Forwards back to client
      â†“
00:39 â”‚  OpenShift Route              â”‚  Forwards response
      â†“
00:40 â”‚  User Browser                 â”‚  JavaScript receives JSON response
      â”‚                               â”‚  React updates UI:
      â”‚                               â”‚  â€¢ Displays answer in chat
      â”‚                               â”‚  â€¢ Shows citations
      â”‚                               â”‚  â€¢ Updates chat history
      â†“
00:40 â”‚  ğŸ‘¤ USER                       â”‚  SEES AI ANSWER! ğŸ‰


TOTAL TIME: ~40 seconds (worst case)
            ~3-5 seconds (with caching and fast LLM)
```

---

## ğŸ“Š Service Connection Matrix

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              WHO TALKS TO WHO (Complete Matrix)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

FROM              TO                        PORT    PROTOCOL  PURPOSE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
User Browser   â†’  OpenShift Route           80/443  HTTP(S)   Access UI
OpenShift Routeâ†’  nginx Service             80      HTTP      Route traffic
nginx Service  â†’  nginx Pod                 80      HTTP      Load balance
nginx Pod      â†’  web-server Service        3000    HTTP      Serve UI
nginx Pod      â†’  api-server Service        8080    HTTP      API calls
web-server Pod â†’  api-server Service        8080    HTTP      Server-side API
api-server Pod â†’  postgresql Service        5432    PostgreSQL Data queries
api-server Pod â†’  redis Service             6379    Redis     Cache/sessions
api-server Pod â†’  vespa Service             8081    HTTP      Search docs
api-server Pod â†’  inference-model-server    9000    HTTP      Embed queries
api-server Pod â†’  vllm-service.vllm-ns     8000    HTTP      LLM inference
api-server Pod â†’  indexing-model-server     9000    HTTP      Embed docs*
vespa Pod      â†’  NFS (via PVC)             -       NFS       Store vectors
inference Pod  â†’  NFS (via PVC)             -       NFS       Load models
indexing Pod   â†’  NFS (via PVC)             -       NFS       Load models
postgresql Pod â†’  NFS (via PVC)             -       NFS       Store DB data

* Only if background workers are deployed (not in minimal setup)
```

---

## ğŸšª Accessing Onyx UI - The Simple Way

### For Testing in OpenShift

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SIMPLEST WAY TO ACCESS ONYX (For Testing)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 1: Deploy Everything
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

oc apply -f storage-setup/01-pv-huggingface-models.yaml
oc apply -f storage-setup/02-pvc-huggingface-models.yaml
oc apply -f 02-postgresql.yaml
oc apply -f 03-vespa.yaml
oc apply -f 04-redis.yaml
oc apply -f 05-configmap.yaml
oc apply -f 06-inference-model-server.yaml
oc apply -f 06-indexing-model-server.yaml
oc apply -f 07-api-server.yaml
oc apply -f 08-web-server.yaml
oc apply -f 09-nginx.yaml

Wait for all pods to be Running:
oc get pods


Step 2: Create OpenShift Route
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

# One command!
oc expose svc/nginx

# Check the route
oc get route nginx

# Output:
# NAME    HOST/PORT                                              PATH   SERVICES
# nginx   nginx-onyx-infra.apps.cluster.company.com             /      nginx


Step 3: Access in Browser
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Open: http://nginx-onyx-infra.apps.cluster.company.com

You should see: Onyx login page! âœ…


Step 4: Login and Test
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Default credentials (if using basic auth):
â€¢ Email: admin@company.com (or create your own)
â€¢ Password: (set via environment variable)

After login:
â€¢ Navigate to Chat
â€¢ Ask a question
â€¢ See AI response!
```

---

## ğŸ” Port Summary (All Services)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ALL PORTS IN THE SYSTEM                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

External (Accessible from Browser):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â€¢ OpenShift Route â†’ NGINX: 80 (HTTP) / 443 (HTTPS)
  URL: http://nginx-onyx-infra.apps.cluster.company.com


Internal (Pod to Pod Communication):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â€¢ nginx â†’ web-server: 3000 (Next.js)
â€¢ nginx â†’ api-server: 8080 (FastAPI)
â€¢ api-server â†’ postgresql: 5432 (PostgreSQL)
â€¢ api-server â†’ redis: 6379 (Redis)
â€¢ api-server â†’ vespa: 8081 (HTTP query), 19071 (HTTP feed)
â€¢ api-server â†’ inference-model-server: 9000 (HTTP)
â€¢ api-server â†’ indexing-model-server: 9000 (HTTP)
â€¢ api-server â†’ vllm (external): 8000 (HTTP)


Storage (NFS):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â€¢ Cluster nodes â†’ NFS server: 2049 (NFS protocol)
  Server: 10.100.50.20:/exports/huggingface-models
```

---

## ğŸ“ Key Concepts for Functional UI

### What You Need for UI to Work

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              MINIMUM REQUIREMENTS FOR FUNCTIONAL UI                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Layer 1: External Access
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… OpenShift Route (created with `oc expose svc/nginx`)
âœ… NGINX Service (ClusterIP)
âœ… NGINX Pod (running)


Layer 2: Frontend
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… Web Server Service
âœ… Web Server Pod (Next.js)
âœ… Can connect to API Server


Layer 3: Backend
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… API Server Service
âœ… API Server Pod (FastAPI)
âœ… Environment variables configured (ConfigMap)


Layer 4: Data Storage
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… PostgreSQL (for users, chat history)
âœ… Redis (for sessions, cache)
âœ… Vespa (for document search)


Layer 5: AI/ML
â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… Inference Model Server (for search embeddings)
âœ… NFS PVC with models (so model server works)
âœ… External LLM configured (vLLM or OpenAI)


If ANY of these is missing â†’ Something won't work!
```

---

## ğŸ› Troubleshooting - What If It Doesn't Work?

### UI Doesn't Load (Blank Page)

```
Symptom: Browser shows nothing or loading forever
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Check:
1. Is route created?
   oc get route nginx
   
2. Is nginx pod running?
   oc get pods -l app=nginx
   
3. Is web-server pod running?
   oc get pods -l app=web-server
   
4. Check nginx logs:
   oc logs deployment/nginx
   
5. Check web-server logs:
   oc logs deployment/web-server
```

### UI Loads but Can't Login

```
Symptom: Login page appears but login fails
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Check:
1. Is api-server pod running?
   oc get pods -l app=api-server
   
2. Is postgresql pod running?
   oc get pods -l app=postgresql
   
3. Check api-server logs:
   oc logs deployment/api-server
   
4. Test API directly:
   oc exec deployment/nginx -- curl http://api-server:8080/health
```

### Can Login but Search Doesn't Work

```
Symptom: Can login, navigate UI, but search/chat fails
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Check:
1. Is vespa pod running?
   oc get pods -l app=vespa
   
2. Is inference-model-server running?
   oc get pods -l app=inference-model-server
   
3. Are models loaded from NFS?
   oc logs deployment/inference-model-server | grep "loaded model"
   Should see: "Loaded model from local cache"
   
4. Is PVC bound?
   oc get pvc huggingface-models-pvc
   Should see: STATUS = Bound
```

### Can Search but LLM Doesn't Respond

```
Symptom: Search works, but AI doesn't generate answers
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Check:
1. Is LLM configured in Onyx UI?
   Settings â†’ LLM Configuration
   
2. Can API server reach external LLM?
   oc exec deployment/api-server -- curl http://vllm-service.vllm-namespace:8000/health
   
3. Check api-server logs for LLM errors:
   oc logs deployment/api-server | grep -i llm
```

---

## âœ… Complete Deployment & Testing Checklist

### Pre-Deployment

- [ ] Get NFS IP and path from colleague
- [ ] Update `storage-setup/01-pv-huggingface-models.yaml` with NFS details
- [ ] Verify you're in correct namespace: `oc project`

### Deploy Infrastructure

```bash
# 1. Storage
oc apply -f storage-setup/01-pv-huggingface-models.yaml
oc apply -f storage-setup/02-pvc-huggingface-models.yaml
oc get pvc huggingface-models-pvc  # Wait for Bound

# 2. Databases
oc apply -f 02-postgresql.yaml
oc apply -f 03-vespa.yaml
oc apply -f 04-redis.yaml

# Wait for ready
oc get pods -w
# Wait for: postgresql, vespa-0, redis all Running
```

### Deploy Application

```bash
# 3. Configuration
oc apply -f 05-configmap.yaml

# 4. Model Servers
oc apply -f 06-inference-model-server.yaml
oc apply -f 06-indexing-model-server.yaml

# Wait for models to load (check logs)
oc logs -f deployment/inference-model-server
# Should see: "Loaded model from local cache" (not "Downloading")

# 5. API & Web
oc apply -f 07-api-server.yaml
oc apply -f 08-web-server.yaml

# 6. NGINX
oc apply -f 09-nginx.yaml
```

### Expose Externally

```bash
# Create route
oc expose svc/nginx

# Get URL
oc get route nginx

# Access in browser
# http://nginx-<namespace>.apps.<cluster-domain>
```

### Verify Each Layer

```bash
# Check all pods running
oc get pods
# All should be: STATUS = Running, READY = 1/1

# Test web server
oc exec deployment/nginx -- curl -s http://web-server:3000 | grep -i onyx

# Test API server
oc exec deployment/nginx -- curl -s http://api-server:8080/health

# Test model server
oc exec deployment/api-server -- curl -s http://inference-model-server:9000/health

# Test database
oc exec deployment/api-server -- curl -s http://postgresql:5432
# (Will fail, but should connect - error is OK)
```

---

## ğŸ¯ Summary

### The Complete Flow (TL;DR)

```
User Browser
    â†“ (HTTP)
OpenShift Route (nginx-onyx-infra.apps.cluster.company.com)
    â†“
NGINX Service (ClusterIP)
    â†“
NGINX Pod (reverse proxy)
    â”œâ”€ / â†’ web-server:3000 (UI)
    â””â”€ /api/* â†’ api-server:8080 (API)
        â†“
    API Server Pod
        â”œâ”€ postgresql:5432 (user data, metadata)
        â”œâ”€ redis:6379 (cache, sessions)
        â”œâ”€ vespa:8081 (vector search)
        â”œâ”€ inference-model-server:9000 (embeddings from NFS models)
        â””â”€ vllm-service.vllm-namespace:8000 (LLM answers)
            â†“
        Response flows back
            â†“
    User sees AI answer! âœ…
```

### Essential Routes

1. **External:** Browser â†’ OpenShift Route (`oc expose svc/nginx`)
2. **Frontend:** NGINX â†’ web-server:3000
3. **API:** NGINX â†’ api-server:8080
4. **Search:** API â†’ vespa:8081
5. **Embeddings:** API â†’ inference-model-server:9000
6. **LLM:** API â†’ vllm-service.vllm-namespace:8000
7. **Models:** inference-model-server â†’ NFS PVC (offline!)

### Critical Components

âœ… OpenShift Route (for external access)
âœ… NGINX (reverse proxy)
âœ… Web Server (UI)
âœ… API Server (backend)
âœ… Inference Model Server (embeddings)
âœ… Vespa (search)
âœ… PostgreSQL (data)
âœ… Redis (cache)
âœ… NFS PVC (models)
âœ… External LLM (answers)

**All connected and working together to deliver AI-powered search!** ğŸ‰

