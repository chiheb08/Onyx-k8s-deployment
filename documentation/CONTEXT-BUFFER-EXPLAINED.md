# Context Handling in Onyx â€“ Buffers, Headroom, and Personas

## ğŸ§  Key Terms

### Buffer
A **buffer** is a small token allowance the system subtracts to stay safe. Onyx pulls a few extra tokens off the top so minor counting errors or unexpected formatting do not overflow the modelâ€™s limit.

- In `compute_max_document_tokens()`, the buffer is **40 tokens** (`_MISC_BUFFER`).
- Example: Model context = 8,000 tokens. After subtracting prompts/user message, 7,200 tokens remain. Subtract 40-token buffer â†’ **7,160 tokens** for documents. That little cushion prevents a â€œcontext length exceededâ€ error.

### Headroom
**Headroom** is the free space that remains after placing the prompt and expected response inside the modelâ€™s context window. Itâ€™s the same idea as buffer but usually largerâ€”extra breathing room.

- Backend headroom: reserving 1,024 tokens for the answer (`GEN_AI_NUM_RESERVED_OUTPUT_TOKENS`).
- Frontend headroom: halving the allowance `(maxTokens * 0.5)` so users canâ€™t unintentionally select too much context.
- Think of headroom as the part you leave blank so the LLM has room to reply.

### Persona
A **persona** is the configuration for an assistant. It bundles system prompt, tools, and model settings (including token-related values).

- `PromptConfig.from_model(persona)` loads the personaâ€™s system prompt.
- `get_llms_for_persona(persona)` returns the LLM + configuration (e.g., max input tokens).
- Personas let different teams use different limits: the â€œFinanceâ€ persona could use a 32K model, the â€œSupport Botâ€ a 4K model.

---

## ğŸ”„ How Onyx Handles Context (Step by Step)

### Example Setup
- Model context window: **8,000 tokens**
- Reserved output tokens: **1,024**
- System prompt: **300** tokens
- Task prompt (persona instructions): **200** tokens
- Expected user message: **512** tokens
- Buffer: **40** tokens

**Backend calculation (`compute_max_document_tokens()`):**
```
Available = 8,000 (model limit)
          - 1,024 (reserved answer space)
          - (300 + 200) (prompts)
          - 512 (expected user input)
          - 40 (buffer)
          = 5,924 tokens
```

So the backend tells the frontend: â€œYou can safely use **5,924** tokens for documents.â€

### Frontend adjustments
1. **Additional headroom**: `ChatPage.tsx` multiplies by 0.5 â†’ 2,962 tokens offered to the UI.
2. **Document selection guard**: `DocumentResults.tsx` disables the â€œSelectâ€ button once document tokens exceed `maxTokens - 75`.
3. **File upload guard**: `ChatInputBar.tsx` compares total attached file tokens to the halved allowance to hide the â€œstill processingâ€ banner.

### Full walk-through
1. **User selects persona â€œLegal Analystâ€** (8K model, reserved output 1,024).
2. **Backend** â†’ `/max-selected-document-tokens` returns **5,924**.
3. **Frontend** sets `availableContextTokens = 2,962` (50% of backend value).
4. User attaches documents: contract (2,000 tokens) + policy memo (600 tokens).
   - Total 2,600 tokens < 2,962 â†’ allowed.
5. User attaches third doc (800 tokens): total = 3,400 tokens > 2,962 â†’ selector greys out; hover message says â€œLLM context limit reached ğŸ˜”â€.
6. User removes policy memo; total = 2,800 tokens â†’ button re-enables.
7. Prompt builder combines:
   - System prompt (300) + task prompt (200)
   - User message (actual size, say 250 tokens)
   - Document chunks (2,800 tokens in this example)
   - Adds buffer (40) â†’ total â‰ˆ 3,590 tokens
   - Leaves 1,024 tokens headroom for the answer.
8. LLM responds within the reserved space, no overflow.

---

## ğŸ› ï¸ Adjusting the Numbers

| Goal | Change |
|------|--------|
| Use full backend allowance | Remove `* 0.5` multiplier in `ChatPage.tsx`. |
| Keep a smaller safety margin | Change multiplier to 0.7 or 0.8. |
| Change buffer size | Modify `_MISC_BUFFER` in `citations_prompt.py`. |
| Reserve more/less reply tokens | Update `GEN_AI_NUM_RESERVED_OUTPUT_TOKENS`. |
| Persona-specific rules | Create/adjust personas to use models with different context windows. |

---

## âœ… Key Takeaways

- **Buffer** â‰ˆ small safety subtraction (40 tokens).
- **Headroom** â‰ˆ the leftover space for the model to answer (backend + frontend guards).
- **Persona** â‰ˆ assistant profile that defines prompts and model limits.
- Onyx applies token rules in layers: backend safe limits â†’ frontend extra guard â†’ prompt builder ensures room for user message + output.

Use these knobs to balance reliability (no context overflow) with flexibility (give users as much context as you trust). When adjusting limits, rebuild the web app so the new behavior is visible immediately.
