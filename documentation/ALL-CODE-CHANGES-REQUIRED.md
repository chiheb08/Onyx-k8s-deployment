# All Code Changes Required - Step by Step

## üìã Overview

This document lists **ALL** code changes needed to fix the deleted files bug. Follow these steps in order.

---

## ‚úÖ Change 1: Add Search Validation (Search Results Fix)

### File: `onyx-repo/backend/onyx/context/search/retrieval/search_runner.py`

#### Step 1.1: Add Imports (at the top of the file)

**Find this section** (around line 1-40):
```python
from onyx.utils.timing import log_function_time
from shared_configs.model_server_models import Embedding

logger = setup_logger()
```

**Replace with**:
```python
from onyx.utils.timing import log_function_time
from shared_configs.model_server_models import Embedding
from onyx.db.models import UserFile
from onyx.db.enums import UserFileStatus

logger = setup_logger()
```

---

#### Step 1.2: Add Validation Function (after `_dedupe_chunks` function)

**Find this function** (around line 45-55):
```python
def _dedupe_chunks(
    chunks: list[InferenceChunkUncleaned],
) -> list[InferenceChunkUncleaned]:
    used_chunks: dict[tuple[str, int], InferenceChunkUncleaned] = {}
    for chunk in chunks:
        key = (chunk.document_id, chunk.chunk_id)
        if key not in used_chunks or (chunk.score or 0) > (
            used_chunks[key].score or 0
        ):
            used_chunks[key] = chunk
    return list(used_chunks.values())
```

**Add this NEW function right after it**:
```python
def _filter_deleted_user_files(
    chunks: list[InferenceChunkUncleaned],
    db_session: Session,
) -> list[InferenceChunkUncleaned]:
    """
    Filter out chunks from deleted user files.
    
    This is a defense-in-depth measure to prevent deleted files
    from appearing in search results if Vespa deletion failed or was delayed.
    """
    # Identify chunks that might be from user files
    # User files use UUID as document_id, connector documents use strings
    user_file_chunks: list[tuple[InferenceChunkUncleaned, UUID]] = []
    other_chunks: list[InferenceChunkUncleaned] = []
    
    for chunk in chunks:
        # Try to parse document_id as UUID
        # User files use UUID as document_id, connector documents use strings
        try:
            user_file_id = UUID(chunk.document_id)
            user_file_chunks.append((chunk, user_file_id))
        except (ValueError, TypeError):
            # Not a UUID, so not a user file - keep it
            other_chunks.append(chunk)
    
    # If no potential user file chunks, return all chunks
    if not user_file_chunks:
        return chunks
    
    # Batch query for all user files to check their status
    user_file_ids = [uf_id for _, uf_id in user_file_chunks]
    valid_user_files = (
        db_session.query(UserFile.id)
        .filter(
            UserFile.id.in_(user_file_ids),
            UserFile.status != UserFileStatus.DELETING,
        )
        .all()
    )
    valid_user_file_ids = {str(uf.id) for uf in valid_user_files}
    
    # Filter chunks: keep only those from valid (non-deleted) user files
    filtered_chunks = other_chunks.copy()
    filtered_count = 0
    for chunk, user_file_id in user_file_chunks:
        if str(user_file_id) in valid_user_file_ids:
            filtered_chunks.append(chunk)
        else:
            filtered_count += 1
            logger.debug(
                f"Filtered out chunk from deleted user_file: {user_file_id} "
                f"(document_id: {chunk.document_id})"
            )
    
    if filtered_count > 0:
        logger.info(
            f"Filtered out {filtered_count} chunk(s) from deleted user files"
        )
    
    return filtered_chunks
```

---

#### Step 1.3: Update `doc_index_retrieval()` - Path 1 (with large chunks)

**Find this code** (around line 320-323):
```python
    # Deduplicate the chunks
    deduped_chunks = list(unique_chunks.values())
    deduped_chunks.sort(key=lambda chunk: chunk.score or 0, reverse=True)
    return cleanup_chunks(deduped_chunks)
```

**Replace with**:
```python
    # Deduplicate the chunks
    deduped_chunks = list(unique_chunks.values())
    deduped_chunks.sort(key=lambda chunk: chunk.score or 0, reverse=True)
    
    # Filter out chunks from deleted user files (defense in depth)
    deduped_chunks = _filter_deleted_user_files(deduped_chunks, db_session)
    
    return cleanup_chunks(deduped_chunks)
```

---

#### Step 1.4: Update `doc_index_retrieval()` - Path 2 (no large chunks)

**Find this code** (around line 281-283):
```python
    # If there are no large chunks, just return the normal chunks
    if not retrieval_requests:
        return cleanup_chunks(normal_chunks)
```

**Replace with**:
```python
    # If there are no large chunks, filter and return the normal chunks
    if not retrieval_requests:
        filtered_chunks = _filter_deleted_user_files(normal_chunks, db_session)
        return cleanup_chunks(filtered_chunks)
```

---

## ‚úÖ Change 2: Add Display Validation (Thinking Window Fix)

### File: `onyx-repo/backend/onyx/agents/agent_search/dr/utils.py`

#### Step 2.1: Add Type Import (at the top of the file)

**Find this section** (around line 1-20):
```python
import copy
import re

from langchain.schema.messages import BaseMessage
from langchain.schema.messages import HumanMessage
```

**Replace with**:
```python
import copy
import re
from typing import TYPE_CHECKING

from langchain.schema.messages import BaseMessage
from langchain.schema.messages import HumanMessage
```

**Then add at the end of imports** (after line 20):
```python
if TYPE_CHECKING:
    from sqlalchemy.orm import Session
```

---

#### Step 2.2: Update `convert_inference_sections_to_search_docs` Function

**Find this function** (around line 264-277):
```python
def convert_inference_sections_to_search_docs(
    inference_sections: list[InferenceSection],
    is_internet: bool = False,
) -> list[SavedSearchDoc]:
    # Convert InferenceSections to SavedSearchDocs
    search_docs = SearchDoc.from_chunks_or_sections(inference_sections)
    for search_doc in search_docs:
        search_doc.is_internet = is_internet

    retrieved_saved_search_docs = [
        SavedSearchDoc.from_search_doc(search_doc, db_doc_id=0)
        for search_doc in search_docs
    ]
    return retrieved_saved_search_docs
```

**Replace with**:
```python
def convert_inference_sections_to_search_docs(
    inference_sections: list[InferenceSection],
    is_internet: bool = False,
    db_session: "Session | None" = None,
) -> list[SavedSearchDoc]:
    """
    Convert InferenceSections to SavedSearchDoc objects.
    
    Optionally filters out deleted user files if db_session is provided.
    This is a defense-in-depth measure to prevent deleted files from appearing
    in the thinking window during search.
    """
    from onyx.utils.logger import setup_logger
    
    logger = setup_logger()
    
    # Convert InferenceSections to SavedSearchDocs
    search_docs = SearchDoc.from_chunks_or_sections(inference_sections)
    for search_doc in search_docs:
        search_doc.is_internet = is_internet

    # Filter out deleted user files if db_session is provided
    if db_session is not None:
        from onyx.db.models import UserFile
        from onyx.db.enums import UserFileStatus
        from uuid import UUID
        
        # Identify user file document IDs (UUIDs)
        user_file_doc_ids: list[tuple[SearchDoc, UUID]] = []
        other_docs: list[SearchDoc] = []
        
        for search_doc in search_docs:
            # Skip internet search docs
            if search_doc.is_internet:
                other_docs.append(search_doc)
                continue
                
            # Try to parse document_id as UUID (user files use UUID)
            try:
                user_file_id = UUID(search_doc.document_id)
                user_file_doc_ids.append((search_doc, user_file_id))
            except (ValueError, TypeError):
                # Not a UUID, so not a user file - keep it
                other_docs.append(search_doc)
        
        # If we have potential user files, validate them
        if user_file_doc_ids:
            user_file_ids = [uf_id for _, uf_id in user_file_doc_ids]
            valid_user_files = (
                db_session.query(UserFile.id)
                .filter(
                    UserFile.id.in_(user_file_ids),
                    UserFile.status != UserFileStatus.DELETING,
                )
                .all()
            )
            valid_user_file_ids = {str(uf.id) for uf in valid_user_files}
            
            # Filter: keep only valid user files
            filtered_docs = other_docs.copy()
            filtered_count = 0
            for search_doc, user_file_id in user_file_doc_ids:
                if str(user_file_id) in valid_user_file_ids:
                    filtered_docs.append(search_doc)
                else:
                    filtered_count += 1
                    logger.debug(
                        f"Filtered out deleted user_file from search display: {user_file_id} "
                        f"(document_id: {search_doc.document_id})"
                    )
            
            if filtered_count > 0:
                logger.info(
                    f"Filtered out {filtered_count} deleted user file(s) from search display"
                )
            
            search_docs = filtered_docs

    retrieved_saved_search_docs = [
        SavedSearchDoc.from_search_doc(search_doc, db_doc_id=0)
        for search_doc in search_docs
    ]
    return retrieved_saved_search_docs
```

---

#### Step 2.3: Update Call Site in `internal_search.py`

### File: `onyx-repo/backend/onyx/tools/tool_implementations_v2/internal_search.py`

**Find this code** (around line 102-113):
```python
                    run_context.context.run_dependencies.emitter.emit(
                        Packet(
                            ind=index,
                            obj=SearchToolDelta(
                                type="internal_search_tool_delta",
                                queries=[],
                                documents=convert_inference_sections_to_search_docs(
                                    retrieved_sections, is_internet=False
                                ),
                            ),
                        )
                    )
```

**Replace with**:
```python
                    run_context.context.run_dependencies.emitter.emit(
                        Packet(
                            ind=index,
                            obj=SearchToolDelta(
                                type="internal_search_tool_delta",
                                queries=[],
                                documents=convert_inference_sections_to_search_docs(
                                    retrieved_sections, 
                                    is_internet=False,
                                    db_session=search_db_session
                                ),
                            ),
                        )
                    )
```

---

## ‚úÖ Change 3: Add Retry Logic to Deletion Task

### File: `onyx-repo/backend/onyx/background/celery/tasks/user_file_processing/tasks.py`

#### Step 3.1: Update Task Decorator

**Find this code** (around line 402-406):
```python
@shared_task(
    name=OnyxCeleryTask.DELETE_SINGLE_USER_FILE,
    bind=True,
    ignore_result=True,
)
def process_single_user_file_delete(
    self: Task, *, user_file_id: str, tenant_id: str
) -> None:
    """Process a single user file delete."""
```

**Replace with**:
```python
@shared_task(
    name=OnyxCeleryTask.DELETE_SINGLE_USER_FILE,
    bind=True,
    ignore_result=True,
    autoretry_for=(Exception,),
    retry_kwargs={'max_retries': 3, 'countdown': 60},
)
def process_single_user_file_delete(
    self: Task, *, user_file_id: str, tenant_id: str
) -> None:
    """Process a single user file delete with retry logic."""
```

---

#### Step 3.2: Add Status Check

**Find this code** (around line 442-448):
```python
            user_file = db_session.get(UserFile, _as_uuid(user_file_id))
            if not user_file:
                task_logger.info(
                    f"process_single_user_file_delete - User file not found id={user_file_id}"
                )
                return None

            # 1) Delete Vespa chunks for the document
```

**Replace with**:
```python
            user_file = db_session.get(UserFile, _as_uuid(user_file_id))
            if not user_file:
                task_logger.info(
                    f"process_single_user_file_delete - User file not found id={user_file_id}"
                )
                return None
            
            # Check if file is still in DELETING status
            if user_file.status != UserFileStatus.DELETING:
                task_logger.warning(
                    f"process_single_user_file_delete - File {user_file_id} is not in DELETING status, "
                    f"current status: {user_file.status}. Skipping deletion."
                )
                return None

            # 1) Delete Vespa chunks for the document
```

---

#### Step 3.3: Add Error Handling for Vespa Deletion

**Find this code** (around line 459-463):
```python
            retry_index.delete_single(
                doc_id=user_file_id,
                tenant_id=tenant_id,
                chunk_count=chunk_count,
            )

            # 2) Delete the user-uploaded file content from filestore (blob + metadata)
```

**Replace with**:
```python
            try:
                retry_index.delete_single(
                    doc_id=user_file_id,
                    tenant_id=tenant_id,
                    chunk_count=chunk_count,
                )
                task_logger.info(
                    f"process_single_user_file_delete - Deleted {chunk_count} chunks from Vespa for {user_file_id}"
                )
            except Exception as vespa_error:
                task_logger.error(
                    f"process_single_user_file_delete - Failed to delete from Vespa for {user_file_id}: {vespa_error}"
                )
                # Re-raise to trigger Celery retry
                raise

            # 2) Delete the user-uploaded file content from filestore (blob + metadata)
```

---

#### Step 3.4: Update File Store Error Handling

**Find this code** (around line 472-476):
```python
            except Exception as e:
                # This block executed only if the file is not found in the filestore
                task_logger.exception(
                    f"process_single_user_file_delete - Error deleting file id={user_file.id} - {e.__class__.__name__}"
                )

            # 3) Finally, delete the UserFile row
```

**Replace with**:
```python
            except Exception as e:
                # This block executed only if the file is not found in the filestore
                task_logger.warning(
                    f"process_single_user_file_delete - Error deleting from file store for {user_file_id}: {e.__class__.__name__}"
                )
                # Don't fail the whole task if file store deletion fails
                # The file may have already been deleted or not exist

            # 3) Finally, delete the UserFile row
```

---

#### Step 3.5: Re-raise Exceptions to Trigger Retry

**Find this code** (around line 484-488):
```python
    except Exception as e:
        task_logger.exception(
            f"process_single_user_file_delete - Error processing file id={user_file_id} - {e.__class__.__name__}"
        )
        return None
```

**Replace with**:
```python
    except Exception as e:
        task_logger.exception(
            f"process_single_user_file_delete - Error processing file id={user_file_id} - {e.__class__.__name__}"
        )
        # Re-raise to trigger Celery retry
        raise
```

---

## üìù Summary Checklist

### Files to Modify

- [ ] `onyx-repo/backend/onyx/context/search/retrieval/search_runner.py`
  - [ ] Add imports (UserFile, UserFileStatus)
  - [ ] Add `_filter_deleted_user_files()` function
  - [ ] Update `doc_index_retrieval()` - Path 1
  - [ ] Update `doc_index_retrieval()` - Path 2

- [ ] `onyx-repo/backend/onyx/agents/agent_search/dr/utils.py`
  - [ ] Add TYPE_CHECKING import
  - [ ] Add Session type import
  - [ ] Update `convert_inference_sections_to_search_docs()` function

- [ ] `onyx-repo/backend/onyx/tools/tool_implementations_v2/internal_search.py`
  - [ ] Update call to `convert_inference_sections_to_search_docs()` to pass `db_session`

- [ ] `onyx-repo/backend/onyx/background/celery/tasks/user_file_processing/tasks.py`
  - [ ] Update task decorator (add retry config)
  - [ ] Add status check before deletion
  - [ ] Add error handling for Vespa deletion
  - [ ] Update file store error handling
  - [ ] Re-raise exceptions to trigger retry

---

## üß™ Testing After Changes

1. **Test Search Results**:
   - Upload file ‚Üí Delete file ‚Üí Search
   - File should NOT appear in results

2. **Test Thinking Window**:
   - Upload file A ‚Üí Delete file A ‚Üí Upload file B ‚Üí Ask question
   - File A should NOT appear in thinking window

3. **Test Multiple Files**:
   - Upload 3 files ‚Üí Delete 1 ‚Üí Search
   - Only 2 files should appear

4. **Check Logs**:
   - Look for "Filtered out X chunk(s) from deleted user files" messages
   - Verify no errors

---

## ‚ö†Ô∏è Important Notes

1. **Backward Compatible**: All changes are backward compatible
2. **Optional Parameters**: `db_session` is optional in `convert_inference_sections_to_search_docs()`
3. **Performance**: Minimal impact (~10-20ms per search)
4. **No Breaking Changes**: Existing code will continue to work

---

## üîç Verification

After making changes, verify:

1. **No Syntax Errors**: Run linter or type checker
2. **Imports Correct**: All imports are at the top of files
3. **Function Signatures**: Match exactly as shown
4. **Indentation**: Python requires correct indentation

---

**Last Updated**: 2024  
**Version**: 1.0

